{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import itertools\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, roc_curve, auc, f1_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "# import warnings\n",
    "# warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read the data\n",
    "\n",
    "Note about the data: The dataset is rather large, and very skewed toward valid transactions. To balance it a bit better, we sample and only keep around 5% of the valid transactions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The data is imbalanced, with 14987 samples in total but only 492 cases of fraud\n"
     ]
    }
   ],
   "source": [
    "with open('creditcard.csv', newline='') as f:\n",
    "    reader = csv.reader(f, delimiter=',')\n",
    "    csv_data = list(reader)[1:]\n",
    "    \n",
    "csv_data = [row for row in csv_data if int(row[-1]) == 1 or random.random() < .05]\n",
    "\n",
    "# Format: Time, V1, ..., V28, Amount, Class\n",
    "data = np.array([[float(x) for x in row[1:-1]] for row in csv_data])\n",
    "times = np.array([float(row[0]) for row in csv_data])\n",
    "labels = np.array([int(row[-1]) for row in csv_data])\n",
    "print('The data is imbalanced, with {} samples in total but only {} cases of fraud'.format(len(labels), sum(labels)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-1.30935976e-01  1.18590069e-01 -2.27898637e-01  1.43754989e-01\n",
      " -8.40391596e-02 -4.11203755e-02 -1.80095560e-01  2.90122110e-02\n",
      " -8.48341442e-02 -1.80792781e-01  1.16248322e-01 -1.93028312e-01\n",
      " -1.21977554e-02 -2.15601424e-01 -3.67546134e-03 -1.27643149e-01\n",
      " -2.03370165e-01 -6.96279400e-02  1.31197147e-02  9.79558169e-03\n",
      "  1.57893112e-02  4.12297868e-03 -4.53991555e-03 -9.26844808e-03\n",
      "  3.63303940e-03  2.88372901e-03  7.25476920e-03  5.36939983e-03\n",
      "  8.65322987e+01]\n",
      "[5.81422266e+00 3.53893634e+00 5.22815921e+00 2.75932610e+00\n",
      " 2.99819153e+00 1.83731886e+00 3.91572881e+00 2.72613856e+00\n",
      " 1.53972757e+00 2.85541253e+00 1.66448352e+00 2.81317897e+00\n",
      " 9.96798873e-01 2.91861745e+00 8.52508745e-01 1.71793044e+00\n",
      " 3.55123593e+00 1.10269812e+00 7.02269898e-01 6.00834679e-01\n",
      " 9.70866692e-01 5.68914456e-01 4.62628746e-01 3.63621957e-01\n",
      " 2.86246268e-01 2.32051644e-01 1.99043866e-01 1.24127675e-01\n",
      " 5.05943367e+04]\n",
      "The PCA features have mean 0 but different variances\n",
      "The transaction amount is not scaled. We should scale it so it does not affect the SVM too much\n"
     ]
    }
   ],
   "source": [
    "print(np.mean(data, axis=0))\n",
    "print(np.var(data, axis=0))\n",
    "print('The PCA features have mean 0 but different variances')\n",
    "print('The transaction amount is not scaled. We should scale it so it does not affect the SVM too much')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 3.88442864e-16  2.15081172e-16 -1.81456682e-17 -3.38356153e-16\n",
      "  2.33030531e-16 -4.18546746e-18  9.87214729e-17 -2.56335807e-16\n",
      " -2.19097184e-16  1.29962098e-15 -1.07497991e-16 -3.43971347e-16\n",
      "  2.20525984e-16  5.92168641e-16  6.47658495e-16  1.64922234e-16\n",
      "  8.39908498e-17  3.92604256e-16 -2.34089862e-18 -1.08585101e-16\n",
      " -6.75712239e-17 -1.42913342e-16 -1.72819063e-16 -3.78497749e-17\n",
      " -8.33974764e-16  4.83106656e-17  1.47692831e-16 -2.01402472e-17\n",
      "  8.58712358e-15]\n",
      "[1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1.]\n"
     ]
    }
   ],
   "source": [
    "scaler = StandardScaler()\n",
    "data_sc = scaler.fit_transform(data)\n",
    "print(np.mean(data_sc, axis=0))\n",
    "print(np.var(data_sc, axis=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preliminary plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1.06937359  1.32270727 -0.24604595  1.15431212 -0.71476402  0.92706025\n",
      " -0.54962614 -0.42607169 -1.16572175]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot the first two principal components\n",
    "data_fraud = data[np.where(labels == 1)]\n",
    "data_valid = data[np.where(labels == 0)]\n",
    "plt.scatter(data_valid[:, 0], data_valid[:, -1], c='b')\n",
    "plt.scatter(data_fraud[:, 0], data_fraud[:, -1], c='r')\n",
    "print(data_valid[1:10, 0])\n",
    "plt.ylim((0, 10000))\n",
    "plt.legend(['valid', 'fraud'])\n",
    "plt.xlabel('First PCA feature')\n",
    "plt.ylabel('Transaction amount')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting up cross-validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "fraud_indices = np.where(labels == 1)[0]\n",
    "np.random.shuffle(fraud_indices)\n",
    "fraud_splits = np.array_split(fraud_indices, 5)\n",
    "\n",
    "valid_indices = np.where(labels == 0)[0]\n",
    "np.random.shuffle(valid_indices)\n",
    "valid_splits = np.array_split(valid_indices, 5)\n",
    "\n",
    "folds = [np.concatenate((fraud_sp, valid_sp)) for fraud_sp, valid_sp in zip(fraud_splits, valid_splits)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_binary_SVM(data, scale=False, kernel=None, class_weight=None):\n",
    "    confusion_mat = np.zeros((2, 2))\n",
    "    for i in range(5):\n",
    "        if kernel is None:\n",
    "            if class_weight is None:\n",
    "                svm = LinearSVC(fit_intercept=False, dual=False)\n",
    "            else:\n",
    "                svm = LinearSVC(fit_intercept=False, dual=False, class_weight=class_weight)\n",
    "        else:\n",
    "            if class_weight is None:\n",
    "                svm = SVC(kernel=kernel, gamma='auto')\n",
    "            else:\n",
    "                svm = SVC(kernel=kernel, gamma='auto', class_weight=class_weight)\n",
    "        \n",
    "        train_data = np.delete(data, folds[i], axis=0)\n",
    "        test_data = data[folds[i]]\n",
    "        if scale:\n",
    "            scaler = StandardScaler()\n",
    "            train_data = scaler.fit_transform(train_data)\n",
    "            test_data = scaler.transform(test_data)\n",
    "\n",
    "        svm.fit(train_data, np.delete(labels, folds[i]))\n",
    "        pred = svm.predict(test_data)\n",
    "\n",
    "        conf_mat = confusion_matrix(labels[folds[i]], pred)\n",
    "        print('Fold', i)\n",
    "        print(conf_mat)\n",
    "\n",
    "        confusion_mat += conf_mat\n",
    "\n",
    "    confusion_mat /= 5\n",
    "    print('Final Confusion Matrix')\n",
    "    print(confusion_mat)\n",
    "    print('False negatives (valid):', confusion_mat[0, 1] / sum(confusion_mat[0, :]))\n",
    "    print('False positives (fraud):', confusion_mat[1, 0] / sum(confusion_mat[1,:]))\n",
    "    fscore = 2*confusion_mat[1,1] / (2*confusion_mat[1,1] + confusion_mat[0,1] + confusion_mat[1,0])\n",
    "    print('F-score: {}'.format(fscore))\n",
    "    return confusion_mat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Linear SVM -- Without scaling or class weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 0\n",
      "[[2894    5]\n",
      " [  20   79]]\n",
      "Fold 1\n",
      "[[2889   10]\n",
      " [  18   81]]\n",
      "Fold 2\n",
      "[[2888   11]\n",
      " [  28   70]]\n",
      "Fold 3\n",
      "[[2888   11]\n",
      " [  19   79]]\n",
      "Fold 4\n",
      "[[2889   10]\n",
      " [  16   82]]\n",
      "Final Confusion Matrix\n",
      "[[2889.6    9.4]\n",
      " [  20.2   78.2]]\n",
      "False negatives (valid): 0.0032424974129010003\n",
      "False positives (fraud): 0.20528455284552843\n",
      "F-score: 0.8408602150537635\n"
     ]
    }
   ],
   "source": [
    "train_binary_SVM(data);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Linear SVM with Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 0\n",
      "[[2899    0]\n",
      " [  16   83]]\n",
      "Fold 1\n",
      "[[2899    0]\n",
      " [  16   83]]\n",
      "Fold 2\n",
      "[[2898    1]\n",
      " [  29   69]]\n",
      "Fold 3\n",
      "[[2899    0]\n",
      " [  19   79]]\n",
      "Fold 4\n",
      "[[2897    2]\n",
      " [  18   80]]\n",
      "Final Confusion Matrix\n",
      "[[2.8984e+03 6.0000e-01]\n",
      " [1.9600e+01 7.8800e+01]]\n",
      "False negatives (valid): 0.00020696791997240426\n",
      "False positives (fraud): 0.1991869918699187\n",
      "F-score: 0.8863892013498313\n"
     ]
    }
   ],
   "source": [
    "train_binary_SVM(data, scale=True);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Linear SVM with Scaling and Class Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 0\n",
      "[[2824   75]\n",
      " [  10   89]]\n",
      "Fold 1\n",
      "[[2814   85]\n",
      " [  10   89]]\n",
      "Fold 2\n",
      "[[2837   62]\n",
      " [  12   86]]\n",
      "Fold 3\n",
      "[[2828   71]\n",
      " [   5   93]]\n",
      "Fold 4\n",
      "[[2809   90]\n",
      " [   7   91]]\n",
      "Final Confusion Matrix\n",
      "[[2822.4   76.6]\n",
      " [   8.8   89.6]]\n",
      "False negatives (valid): 0.026422904449810276\n",
      "False positives (fraud): 0.0894308943089431\n",
      "F-score: 0.6772486772486773\n"
     ]
    }
   ],
   "source": [
    "train_binary_SVM(data, scale=True, class_weight='balanced');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Kernel SVM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### RBF Kernel with Scaling and Class Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 0\n",
      "[[2899    0]\n",
      " [  22   77]]\n",
      "Fold 1\n",
      "[[2899    0]\n",
      " [  22   77]]\n",
      "Fold 2\n",
      "[[2898    1]\n",
      " [  30   68]]\n",
      "Fold 3\n",
      "[[2899    0]\n",
      " [  19   79]]\n",
      "Fold 4\n",
      "[[2897    2]\n",
      " [  15   83]]\n",
      "Final Confusion Matrix\n",
      "[[2.8984e+03 6.0000e-01]\n",
      " [2.1600e+01 7.6800e+01]]\n",
      "False negatives (valid): 0.00020696791997240426\n",
      "False positives (fraud): 0.21951219512195122\n",
      "F-score: 0.8737201365187713\n"
     ]
    }
   ],
   "source": [
    "train_binary_SVM(data, scale=True, kernel='rbf');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 0\n",
      "[[2890    9]\n",
      " [  45   54]]\n",
      "Fold 1\n",
      "[[2889   10]\n",
      " [  50   49]]\n",
      "Fold 2\n",
      "[[2892    7]\n",
      " [  50   48]]\n",
      "Fold 3\n",
      "[[2889   10]\n",
      " [  48   50]]\n",
      "Fold 4\n",
      "[[2890    9]\n",
      " [  42   56]]\n",
      "Final Confusion Matrix\n",
      "[[2890.     9. ]\n",
      " [  47.    51.4]]\n",
      "False negatives (valid): 0.003104518799586064\n",
      "False positives (fraud): 0.4776422764227642\n",
      "F-score: 0.6473551637279596\n"
     ]
    }
   ],
   "source": [
    "train_binary_SVM(data, kernel='rbf', class_weight='balanced');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Polynomial Kernel with Scaling and Class Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 0\n",
      "[[2897    2]\n",
      " [  16   83]]\n",
      "Fold 1\n",
      "[[2896    3]\n",
      " [  16   83]]\n",
      "Fold 2\n",
      "[[2897    2]\n",
      " [  26   72]]\n",
      "Fold 3\n",
      "[[2894    5]\n",
      " [  16   82]]\n",
      "Fold 4\n",
      "[[2895    4]\n",
      " [  18   80]]\n",
      "Final Confusion Matrix\n",
      "[[2895.8    3.2]\n",
      " [  18.4   80. ]]\n",
      "False negatives (valid): 0.0011038289065194895\n",
      "False positives (fraud): 0.18699186991869915\n",
      "F-score: 0.881057268722467\n"
     ]
    }
   ],
   "source": [
    "train_binary_SVM(data, scale=True, kernel='poly');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 0\n",
      "[[2877   22]\n",
      " [  13   86]]\n",
      "Fold 1\n",
      "[[2868   31]\n",
      " [  15   84]]\n",
      "Fold 2\n",
      "[[2884   15]\n",
      " [  18   80]]\n",
      "Fold 3\n",
      "[[2876   23]\n",
      " [  10   88]]\n",
      "Fold 4\n",
      "[[2878   21]\n",
      " [  16   82]]\n",
      "Final Confusion Matrix\n",
      "[[2876.6   22.4]\n",
      " [  14.4   84. ]]\n",
      "False negatives (valid): 0.007726802345636426\n",
      "False positives (fraud): 0.14634146341463414\n",
      "F-score: 0.8203125\n"
     ]
    }
   ],
   "source": [
    "train_binary_SVM(data, scale=True, kernel='poly', class_weight='balanced');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interestingly, balanced weights peform worse than only using scaled data. The balanced weights overcompensate too much.\n",
    "Simply scaling the data works best, with the poly kernel (and pretty well with RBF and linear)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_logistic_regression(data, folds, labels, class_weight = None, C=1.0):\n",
    "    err = 0\n",
    "    err_valid = 0\n",
    "    err_fraud = 0\n",
    "    confusion_mat = np.zeros((2, 2))\n",
    "    AU = 0\n",
    "    for i in range(5):\n",
    "        model = LogisticRegression(class_weight=class_weight, C=C)\n",
    "        \n",
    "        model.fit(np.delete(data, folds[i], axis=0), np.delete(labels, folds[i]))\n",
    "        pred = model.predict(data[folds[i]])\n",
    "        pred1 = model.predict_proba(data[folds[i]])[:,1]\n",
    "        conf_mat = confusion_matrix(labels[folds[i]], pred)\n",
    "        fpr, tpr, thresholds = roc_curve(labels[folds[i]], pred1)\n",
    "        area_under_curve = auc(fpr, tpr)\n",
    "        AU += area_under_curve\n",
    "        #print('Fold', i)\n",
    "        #print(conf_mat)\n",
    "        #print('Area under curve:', area_under_curve)\n",
    "\n",
    "        confusion_mat += conf_mat\n",
    "    AU/=5\n",
    "    confusion_mat /= 5\n",
    "    print('Final Confusion Matrix')\n",
    "    print(confusion_mat)\n",
    "    return confusion_mat, AU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Logistic Regression with Scaling, Class Weights, and Regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking class weight None and coefficient 0.01\n",
      "Final Confusion Matrix\n",
      "[[2.8984e+03 6.0000e-01]\n",
      " [1.8200e+01 8.0200e+01]]\n",
      "F1 score: 0.8950892857142857\n",
      "Recall: 0.8150406504065041\n",
      "Precision: 0.9925742574257427\n",
      "AUC: 0.979873973275639\n",
      "Checking class weight None and coefficient 0.016681005372000592\n",
      "Final Confusion Matrix\n",
      "[[2.8984e+03 6.0000e-01]\n",
      " [1.7400e+01 8.1000e+01]]\n",
      "F1 score: 0.9\n",
      "Recall: 0.823170731707317\n",
      "Precision: 0.9926470588235294\n",
      "AUC: 0.9802891535114469\n",
      "Checking class weight None and coefficient 0.027825594022071243\n",
      "Final Confusion Matrix\n",
      "[[2.8982e+03 8.0000e-01]\n",
      " [1.7200e+01 8.1200e+01]]\n",
      "F1 score: 0.9002217294900221\n",
      "Recall: 0.8252032520325203\n",
      "Precision: 0.9902439024390244\n",
      "AUC: 0.9806583408761501\n",
      "Checking class weight None and coefficient 0.046415888336127774\n",
      "Final Confusion Matrix\n",
      "[[2.8982e+03 8.0000e-01]\n",
      " [1.7200e+01 8.1200e+01]]\n",
      "F1 score: 0.9002217294900221\n",
      "Recall: 0.8252032520325203\n",
      "Precision: 0.9902439024390244\n",
      "AUC: 0.9809973783067955\n",
      "Checking class weight None and coefficient 0.0774263682681127\n",
      "Final Confusion Matrix\n",
      "[[2.898e+03 1.000e+00]\n",
      " [1.740e+01 8.100e+01]]\n",
      "F1 score: 0.8980044345898003\n",
      "Recall: 0.823170731707317\n",
      "Precision: 0.9878048780487805\n",
      "AUC: 0.9812745941509554\n",
      "Checking class weight None and coefficient 0.1291549665014884\n",
      "Final Confusion Matrix\n",
      "[[2.8976e+03 1.4000e+00]\n",
      " [1.6800e+01 8.1600e+01]]\n",
      "F1 score: 0.8996692392502755\n",
      "Recall: 0.8292682926829268\n",
      "Precision: 0.9831325301204819\n",
      "AUC: 0.9813873790811651\n",
      "Checking class weight None and coefficient 0.21544346900318834\n",
      "Final Confusion Matrix\n",
      "[[2.8974e+03 1.6000e+00]\n",
      " [1.7000e+01 8.1400e+01]]\n",
      "F1 score: 0.8974641675854466\n",
      "Recall: 0.8272357723577236\n",
      "Precision: 0.9807228915662651\n",
      "AUC: 0.9813952792171883\n",
      "Checking class weight None and coefficient 0.3593813663804626\n",
      "Final Confusion Matrix\n",
      "[[2.8972e+03 1.8000e+00]\n",
      " [1.6800e+01 8.1600e+01]]\n",
      "F1 score: 0.8976897689768977\n",
      "Recall: 0.8292682926829268\n",
      "Precision: 0.9784172661870504\n",
      "AUC: 0.9812648807523887\n",
      "Checking class weight None and coefficient 0.5994842503189409\n",
      "Final Confusion Matrix\n",
      "[[2.8972e+03 1.8000e+00]\n",
      " [1.6600e+01 8.1800e+01]]\n",
      "F1 score: 0.8989010989010988\n",
      "Recall: 0.83130081300813\n",
      "Precision: 0.9784688995215312\n",
      "AUC: 0.9810895809294271\n",
      "Checking class weight None and coefficient 1.0\n",
      "Final Confusion Matrix\n",
      "[[2.8972e+03 1.8000e+00]\n",
      " [1.6200e+01 8.2200e+01]]\n",
      "F1 score: 0.9013157894736842\n",
      "Recall: 0.8353658536585366\n",
      "Precision: 0.9785714285714286\n",
      "AUC: 0.9808823036881973\n",
      "Checking class weight balanced and coefficient 0.01\n",
      "Final Confusion Matrix\n",
      "[[2838.8   60.2]\n",
      " [   9.4   89. ]]\n",
      "F1 score: 0.7189014539579969\n",
      "Recall: 0.9044715447154471\n",
      "Precision: 0.5965147453083111\n",
      "AUC: 0.9800391223837732\n",
      "Checking class weight balanced and coefficient 0.016681005372000592\n",
      "Final Confusion Matrix\n",
      "[[2835.    64. ]\n",
      " [   9.2   89.2]]\n",
      "F1 score: 0.7090620031796503\n",
      "Recall: 0.9065040650406504\n",
      "Precision: 0.5822454308093995\n",
      "AUC: 0.9800228883508832\n",
      "Checking class weight balanced and coefficient 0.027825594022071243\n",
      "Final Confusion Matrix\n",
      "[[2833.8   65.2]\n",
      " [   9.    89.4]]\n",
      "F1 score: 0.7067193675889328\n",
      "Recall: 0.9085365853658537\n",
      "Precision: 0.5782664941785252\n",
      "AUC: 0.9799925464243209\n",
      "Checking class weight balanced and coefficient 0.046415888336127774\n",
      "Final Confusion Matrix\n",
      "[[2832.8   66.2]\n",
      " [   9.    89.4]]\n",
      "F1 score: 0.7039370078740158\n",
      "Recall: 0.9085365853658537\n",
      "Precision: 0.5745501285347043\n",
      "AUC: 0.9798996789387564\n",
      "Checking class weight balanced and coefficient 0.0774263682681127\n",
      "Final Confusion Matrix\n",
      "[[2830.6   68.4]\n",
      " [   9.    89.4]]\n",
      "F1 score: 0.6978922716627635\n",
      "Recall: 0.9085365853658537\n",
      "Precision: 0.5665399239543726\n",
      "AUC: 0.9798328904350685\n",
      "Checking class weight balanced and coefficient 0.1291549665014884\n",
      "Final Confusion Matrix\n",
      "[[2830.2   68.8]\n",
      " [   9.    89.4]]\n",
      "F1 score: 0.6968043647700702\n",
      "Recall: 0.9085365853658537\n",
      "Precision: 0.5651074589127687\n",
      "AUC: 0.9798044968768863\n",
      "Checking class weight balanced and coefficient 0.21544346900318834\n",
      "Final Confusion Matrix\n",
      "[[2830.    69. ]\n",
      " [   8.8   89.6]]\n",
      "F1 score: 0.6972762645914397\n",
      "Recall: 0.910569105691057\n",
      "Precision: 0.5649432534678436\n",
      "AUC: 0.9797790863133592\n",
      "Checking class weight balanced and coefficient 0.3593813663804626\n",
      "Final Confusion Matrix\n",
      "[[2829.2   69.8]\n",
      " [   8.8   89.6]]\n",
      "F1 score: 0.6951124903025602\n",
      "Recall: 0.910569105691057\n",
      "Precision: 0.5621079046424091\n",
      "AUC: 0.9797360977694097\n",
      "Checking class weight balanced and coefficient 0.5994842503189409\n",
      "Final Confusion Matrix\n",
      "[[2828.4   70.6]\n",
      " [   8.6   89.8]]\n",
      "F1 score: 0.6939721792890263\n",
      "Recall: 0.9126016260162602\n",
      "Precision: 0.559850374064838\n",
      "AUC: 0.9797412140141161\n",
      "Checking class weight balanced and coefficient 1.0\n",
      "Final Confusion Matrix\n",
      "[[2828.2   70.8]\n",
      " [   8.6   89.8]]\n",
      "F1 score: 0.6934362934362934\n",
      "Recall: 0.9126016260162602\n",
      "Precision: 0.5591531755915318\n",
      "AUC: 0.9797640006800801\n",
      "Best parameters found for Logistic Regression under AUC metric is None class weights and 0.21544346900318834 coefficient\n",
      "Best parameters found for Logistic Regression under F1 metric is None class weights and 0.01 coefficient\n",
      "Best parameters found for Logistic Regression under precision metric is None class weights and 0.016681005372000592 coefficient\n",
      "Best parameters found for Logistic Regression under recall metric is balanced class weights and 0.5994842503189409 coefficient\n"
     ]
    }
   ],
   "source": [
    "weights = [None,'balanced']\n",
    "coefficients = np.logspace(-2,0,10)\n",
    "fscore = []\n",
    "recall = []\n",
    "precision = []\n",
    "AU = []\n",
    "param_permuations = list(itertools.product(weights, coefficients))\n",
    "for cw,C in param_permuations:\n",
    "    print('Checking class weight {} and coefficient {}'.format(cw, C))\n",
    "    cf,au = train_logistic_regression(data_sc, folds, labels, cw, C)\n",
    "    TP = cf[1][1]\n",
    "    FP = cf[0][1]\n",
    "    FN = cf[1][0]\n",
    "    prec = TP/(TP+FP)\n",
    "    rec = TP/(TP+FN)\n",
    "    f1 = prec*rec*2/(prec+rec)\n",
    "    print('F1 score:', f1)\n",
    "    print('Recall:', rec)\n",
    "    print('Precision:', prec)\n",
    "    print('AUC:', au)\n",
    "    fscore.append(f1)\n",
    "    recall.append(rec)\n",
    "    precision.append(prec)\n",
    "    AU.append(au)\n",
    "\n",
    "idx = np.argmax(AU)\n",
    "best_params = param_permuations[idx]\n",
    "print('Best parameters found for Logistic Regression under AUC metric is {} class weights and {} coefficient'.format(best_params[0], best_params[1]))\n",
    "\n",
    "idx = np.argmax(f1)\n",
    "best_params = param_permuations[idx]\n",
    "print('Best parameters found for Logistic Regression under F1 metric is {} class weights and {} coefficient'.format(best_params[0], best_params[1]))\n",
    "\n",
    "idx = np.argmax(precision)\n",
    "best_params = param_permuations[idx]\n",
    "print('Best parameters found for Logistic Regression under precision metric is {} class weights and {} coefficient'.format(best_params[0], best_params[1]))\n",
    "\n",
    "idx = np.argmax(recall)\n",
    "best_params = param_permuations[idx]\n",
    "print('Best parameters found for Logistic Regression under recall metric is {} class weights and {} coefficient'.format(best_params[0], best_params[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice how when using balanced class weights, the metric of precision is thrown off since while there is a small false positive rate, the difference in size of the two distributions cause false positives to be on at least the same order of magnitude as the true positives. We can rectify this by downsampling until the two classes have around equal representation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking class weight None and coefficient 0.01\n",
      "Final Confusion Matrix\n",
      "[[101.8   0.2]\n",
      " [ 19.4  79. ]]\n",
      "F1 score: 0.8896396396396395\n",
      "Recall: 0.8028455284552845\n",
      "Precision: 0.9974747474747474\n",
      "AUC: 0.9772260419319242\n",
      "Checking class weight None and coefficient 0.016681005372000592\n",
      "Final Confusion Matrix\n",
      "[[101.8   0.2]\n",
      " [ 18.2  80.2]]\n",
      "F1 score: 0.8970917225950783\n",
      "Recall: 0.8150406504065041\n",
      "Precision: 0.9975124378109452\n",
      "AUC: 0.9787446291647971\n",
      "Checking class weight None and coefficient 0.027825594022071243\n",
      "Final Confusion Matrix\n",
      "[[101.6   0.4]\n",
      " [ 16.6  81.8]]\n",
      "F1 score: 0.9058693244739755\n",
      "Recall: 0.83130081300813\n",
      "Precision: 0.9951338199513381\n",
      "AUC: 0.9794028722600151\n",
      "Checking class weight None and coefficient 0.046415888336127774\n",
      "Final Confusion Matrix\n",
      "[[101.4   0.6]\n",
      " [ 15.   83.4]]\n",
      "F1 score: 0.9144736842105262\n",
      "Recall: 0.8475609756097561\n",
      "Precision: 0.9928571428571429\n",
      "AUC: 0.9795809232784023\n",
      "Checking class weight None and coefficient 0.0774263682681127\n",
      "Final Confusion Matrix\n",
      "[[100.8   1.2]\n",
      " [ 13.6  84.8]]\n",
      "F1 score: 0.9197396963123644\n",
      "Recall: 0.8617886178861789\n",
      "Precision: 0.9860465116279069\n",
      "AUC: 0.9793592184348487\n",
      "Checking class weight None and coefficient 0.1291549665014884\n",
      "Final Confusion Matrix\n",
      "[[100.4   1.6]\n",
      " [ 12.2  86.2]]\n",
      "F1 score: 0.9258861439312567\n",
      "Recall: 0.8760162601626016\n",
      "Precision: 0.9817767653758542\n",
      "AUC: 0.9786993585312913\n",
      "Checking class weight None and coefficient 0.21544346900318834\n",
      "Final Confusion Matrix\n",
      "[[100.    2. ]\n",
      " [ 11.   87.4]]\n",
      "F1 score: 0.9307774227902023\n",
      "Recall: 0.8882113821138211\n",
      "Precision: 0.9776286353467561\n",
      "AUC: 0.9781379218354009\n",
      "Checking class weight None and coefficient 0.3593813663804626\n",
      "Final Confusion Matrix\n",
      "[[99.4  2.6]\n",
      " [10.6 87.8]]\n",
      "F1 score: 0.9300847457627119\n",
      "Recall: 0.8922764227642277\n",
      "Precision: 0.9712389380530974\n",
      "AUC: 0.977677535660729\n",
      "Checking class weight None and coefficient 0.5994842503189409\n",
      "Final Confusion Matrix\n",
      "[[99.2  2.8]\n",
      " [10.2 88.2]]\n",
      "F1 score: 0.931362196409715\n",
      "Recall: 0.8963414634146342\n",
      "Precision: 0.9692307692307692\n",
      "AUC: 0.9769368353401967\n",
      "Checking class weight None and coefficient 1.0\n",
      "Final Confusion Matrix\n",
      "[[98.8  3.2]\n",
      " [ 9.8 88.6]]\n",
      "F1 score: 0.9316508937960042\n",
      "Recall: 0.9004065040650406\n",
      "Precision: 0.9651416122004357\n",
      "AUC: 0.9765344521647042\n",
      "Checking class weight balanced and coefficient 0.01\n",
      "Final Confusion Matrix\n",
      "[[101.8   0.2]\n",
      " [ 19.4  79. ]]\n",
      "F1 score: 0.8896396396396395\n",
      "Recall: 0.8028455284552845\n",
      "Precision: 0.9974747474747474\n",
      "AUC: 0.9772062360297653\n",
      "Checking class weight balanced and coefficient 0.016681005372000592\n",
      "Final Confusion Matrix\n",
      "[[101.8   0.2]\n",
      " [ 18.   80.4]]\n",
      "F1 score: 0.8983240223463687\n",
      "Recall: 0.8170731707317074\n",
      "Precision: 0.9975186104218362\n",
      "AUC: 0.9788244590765599\n",
      "Checking class weight balanced and coefficient 0.027825594022071243\n",
      "Final Confusion Matrix\n",
      "[[101.6   0.4]\n",
      " [ 16.6  81.8]]\n",
      "F1 score: 0.9058693244739755\n",
      "Recall: 0.83130081300813\n",
      "Precision: 0.9951338199513381\n",
      "AUC: 0.9794624920675341\n",
      "Checking class weight balanced and coefficient 0.046415888336127774\n",
      "Final Confusion Matrix\n",
      "[[101.2   0.8]\n",
      " [ 14.6  83.8]]\n",
      "F1 score: 0.9158469945355192\n",
      "Recall: 0.8516260162601627\n",
      "Precision: 0.9905437352245863\n",
      "AUC: 0.9795211013698409\n",
      "Checking class weight balanced and coefficient 0.0774263682681127\n",
      "Final Confusion Matrix\n",
      "[[100.6   1.4]\n",
      " [ 13.4  85. ]]\n",
      "F1 score: 0.9199134199134199\n",
      "Recall: 0.8638211382113821\n",
      "Precision: 0.9837962962962963\n",
      "AUC: 0.9792795906241285\n",
      "Checking class weight balanced and coefficient 0.1291549665014884\n",
      "Final Confusion Matrix\n",
      "[[100.4   1.6]\n",
      " [ 12.2  86.2]]\n",
      "F1 score: 0.9258861439312567\n",
      "Recall: 0.8760162601626016\n",
      "Precision: 0.9817767653758542\n",
      "AUC: 0.9786593425248888\n",
      "Checking class weight balanced and coefficient 0.21544346900318834\n",
      "Final Confusion Matrix\n",
      "[[100.    2. ]\n",
      " [ 11.   87.4]]\n",
      "F1 score: 0.9307774227902023\n",
      "Recall: 0.8882113821138211\n",
      "Precision: 0.9776286353467561\n",
      "AUC: 0.9780780999268395\n",
      "Checking class weight balanced and coefficient 0.3593813663804626\n",
      "Final Confusion Matrix\n",
      "[[99.2  2.8]\n",
      " [10.4 88. ]]\n",
      "F1 score: 0.9302325581395349\n",
      "Recall: 0.8943089430894309\n",
      "Precision: 0.9691629955947136\n",
      "AUC: 0.9776975436639302\n",
      "Checking class weight balanced and coefficient 0.5994842503189409\n",
      "Final Confusion Matrix\n",
      "[[99.   3. ]\n",
      " [10.2 88.2]]\n",
      "F1 score: 0.9303797468354429\n",
      "Recall: 0.8963414634146342\n",
      "Precision: 0.9671052631578947\n",
      "AUC: 0.9769566412423554\n",
      "Checking class weight balanced and coefficient 1.0\n",
      "Final Confusion Matrix\n",
      "[[98.8  3.2]\n",
      " [ 9.6 88.8]]\n",
      "F1 score: 0.9327731092436974\n",
      "Recall: 0.9024390243902439\n",
      "Precision: 0.9652173913043478\n",
      "AUC: 0.9764944361583018\n",
      "Best parameters found for Logistic Regression under AUC metric is None class weights and 0.046415888336127774 coefficient\n",
      "Best parameters found for Logistic Regression under F1 metric is None class weights and 0.01 coefficient\n",
      "Best parameters found for Logistic Regression under precision metric is balanced class weights and 0.016681005372000592 coefficient\n",
      "Best parameters found for Logistic Regression under recall metric is balanced class weights and 1.0 coefficient\n"
     ]
    }
   ],
   "source": [
    "#Notice that if we fully downsample the dataset so that valid and fraud are equally represented, we have much more consistent metrics.\n",
    "#Then we see that the recommended parameters begin to converge.\n",
    "\n",
    "csv_data2 = [row for row in csv_data if int(row[-1]) == 1 or random.random() < 0.00164/.05]\n",
    "\n",
    "# Find new splits for resampled data\n",
    "data2 = np.array([[float(x) for x in row[1:-1]] for row in csv_data2])\n",
    "times2 = np.array([float(row[0]) for row in csv_data2])\n",
    "labels2 = np.array([int(row[-1]) for row in csv_data2])\n",
    "\n",
    "data_sc2 = scaler.fit_transform(data2)\n",
    "\n",
    "fraud_indices2 = np.where(labels2 == 1)[0]\n",
    "np.random.shuffle(fraud_indices2)\n",
    "fraud_splits2 = np.array_split(fraud_indices2, 5)\n",
    "\n",
    "valid_indices2 = np.where(labels2 == 0)[0]\n",
    "np.random.shuffle(valid_indices2)\n",
    "valid_splits2 = np.array_split(valid_indices2, 5)\n",
    "\n",
    "folds2 = [np.concatenate((fraud_sp, valid_sp)) for fraud_sp, valid_sp in zip(fraud_splits2, valid_splits2)]\n",
    "\n",
    "weights = [None,'balanced']\n",
    "coefficients = np.logspace(-2,0,10)\n",
    "fscore = []\n",
    "recall = []\n",
    "precision = []\n",
    "AU = []\n",
    "param_permuations = list(itertools.product(weights, coefficients))\n",
    "for cw,C in param_permuations:\n",
    "    print('Checking class weight {} and coefficient {}'.format(cw, C))\n",
    "    cf,au = train_logistic_regression(data_sc2, folds2, labels2, cw, C)\n",
    "    TP = cf[1][1]\n",
    "    FP = cf[0][1]\n",
    "    FN = cf[1][0]\n",
    "    prec = TP/(TP+FP)\n",
    "    rec = TP/(TP+FN)\n",
    "    f1 = prec*rec*2/(prec+rec)\n",
    "    print('F1 score:', f1)\n",
    "    print('Recall:', rec)\n",
    "    print('Precision:', prec)\n",
    "    print('AUC:', au)\n",
    "    fscore.append(f1)\n",
    "    recall.append(rec)\n",
    "    precision.append(prec)\n",
    "    AU.append(au)\n",
    "\n",
    "idx = np.argmax(AU)\n",
    "best_params = param_permuations[idx]\n",
    "print('Best parameters found for Logistic Regression under AUC metric is {} class weights and {} coefficient'.format(best_params[0], best_params[1]))\n",
    "\n",
    "idx = np.argmax(f1)\n",
    "best_params = param_permuations[idx]\n",
    "print('Best parameters found for Logistic Regression under F1 metric is {} class weights and {} coefficient'.format(best_params[0], best_params[1]))\n",
    "\n",
    "idx = np.argmax(precision)\n",
    "best_params = param_permuations[idx]\n",
    "print('Best parameters found for Logistic Regression under precision metric is {} class weights and {} coefficient'.format(best_params[0], best_params[1]))\n",
    "\n",
    "idx = np.argmax(recall)\n",
    "best_params = param_permuations[idx]\n",
    "print('Best parameters found for Logistic Regression under recall metric is {} class weights and {} coefficient'.format(best_params[0], best_params[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In these cases, the balanced and None class weights perform around the same as expected. However, we can see that the issue with precision and F1 score is fixed. Thus, we advise caution simply using balanced class weights and just looking at the statistics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# kNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_KNN(data, k, scale=False):\n",
    "    confusion_mat = np.zeros((2, 2))\n",
    "    AU = 0\n",
    "    errors = 0\n",
    "    for i in range(5):\n",
    "        knn = KNeighborsClassifier(n_neighbors=k)\n",
    "        \n",
    "        train_data = np.delete(data, folds[i], axis=0)\n",
    "        test_data = data[folds[i]]\n",
    "        if scale:\n",
    "            scaler = StandardScaler()\n",
    "            train_data = scaler.fit_transform(train_data)\n",
    "            test_data = scaler.transform(test_data)\n",
    "\n",
    "        knn.fit(train_data, np.delete(labels, folds[i]))\n",
    "        pred = knn.predict(test_data)\n",
    "\n",
    "        conf_mat = confusion_matrix(labels[folds[i]], pred)\n",
    "        fpr, tpr, thresholds = roc_curve(labels[folds[i]], pred)\n",
    "        area_under_curve = auc(fpr, tpr)\n",
    "        AU += area_under_curve\n",
    "        err = sum([1 if a != b else 0 for a, b in zip(labels[folds[i]], pred)]) / len(pred)\n",
    "        errors += err\n",
    "\n",
    "        confusion_mat += conf_mat\n",
    "\n",
    "    confusion_mat /= 5\n",
    "    AU /= 5\n",
    "    errors /= 5\n",
    "    return confusion_mat, AU, errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "K = 1, errors = 0.010879150369688397\n",
      "Confusion Matrix:\n",
      "[[2831.8   11.2]\n",
      " [  20.8   77.6]]\n",
      "False Negative Rate: 0.003940\n",
      "False Positive Rate: 0.211382\n",
      "fscore: 0.829059829059829\n",
      "\n",
      "K = 3, errors = 0.008907240076824733\n",
      "Confusion Matrix:\n",
      "[[2.8408e+03 2.2000e+00]\n",
      " [2.4000e+01 7.4400e+01]]\n",
      "False Negative Rate: 0.000774\n",
      "False Positive Rate: 0.243902\n",
      "fscore: 0.8502857142857143\n",
      "\n",
      "K = 5, errors = 0.00938322240870822\n",
      "Confusion Matrix:\n",
      "[[2.8416e+03 1.4000e+00]\n",
      " [2.6200e+01 7.2200e+01]]\n",
      "False Negative Rate: 0.000492\n",
      "False Positive Rate: 0.266260\n",
      "fscore: 0.8395348837209302\n",
      "\n",
      "K = 7, errors = 0.011151166690667654\n",
      "Confusion Matrix:\n",
      "[[2.8416e+03 1.4000e+00]\n",
      " [3.1400e+01 6.7000e+01]]\n",
      "False Negative Rate: 0.000492\n",
      "False Positive Rate: 0.319106\n",
      "fscore: 0.803357314148681\n",
      "\n",
      "K = 9, errors = 0.011627172137466248\n",
      "Confusion Matrix:\n",
      "[[2.8418e+03 1.2000e+00]\n",
      " [3.3000e+01 6.5400e+01]]\n",
      "False Negative Rate: 0.000422\n",
      "False Positive Rate: 0.335366\n",
      "fscore: 0.7927272727272728\n",
      "\n",
      "K = 11, errors = 0.012103062009689311\n",
      "Confusion Matrix:\n",
      "[[2.8418e+03 1.2000e+00]\n",
      " [3.4400e+01 6.4000e+01]]\n",
      "False Negative Rate: 0.000422\n",
      "False Positive Rate: 0.349593\n",
      "fscore: 0.78239608801956\n",
      "\n",
      "The optimal K is 3 with fscore 0.8502857142857143\n"
     ]
    }
   ],
   "source": [
    "fscores = []\n",
    "Ks = [1, 3, 5, 7, 9, 11]\n",
    "for k in Ks:\n",
    "    conf, au, errors = train_KNN(data, k)\n",
    "    print('K = {}, errors = {}'.format(k, errors))\n",
    "    print('Confusion Matrix:')\n",
    "    print(conf)\n",
    "    fnr = conf[0, 1] / sum(conf[0, :])\n",
    "    fpr = conf[1, 0] / sum(conf[1,:])\n",
    "    print('False Negative Rate: {:3f}\\nFalse Positive Rate: {:3f}'.format(fnr, fpr))\n",
    "    fscore = 2*conf[1,1] / (2*conf[1,1] + conf[0,1] + conf[1,0])\n",
    "    print('fscore: {}'.format(fscore))\n",
    "    fscores.append(fscore)\n",
    "    print()\n",
    "    \n",
    "opt_ind = np.argmax(fscores)\n",
    "k_opt = Ks[opt_ind]\n",
    "print('The optimal K is {} with fscore {}'.format(k_opt, fscores[opt_ind]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
