{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import itertools\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, roc_curve, auc, f1_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "# import warnings\n",
    "# warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read the data\n",
    "\n",
    "Note about the data: The dataset is rather large, and very skewed toward valid transactions. To balance it a bit better, we sample and only keep around 5% of the valid transactions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The data is imbalanced, with 14987 samples in total but only 492 cases of fraud\n"
     ]
    }
   ],
   "source": [
    "with open('creditcard.csv', newline='') as f:\n",
    "    reader = csv.reader(f, delimiter=',')\n",
    "    csv_data = list(reader)[1:]\n",
    "    \n",
    "csv_data = [row for row in csv_data if int(row[-1]) == 1 or random.random() < .05]\n",
    "\n",
    "# Format: Time, V1, ..., V28, Amount, Class\n",
    "data = np.array([[float(x) for x in row[1:-1]] for row in csv_data])\n",
    "times = np.array([float(row[0]) for row in csv_data])\n",
    "labels = np.array([int(row[-1]) for row in csv_data])\n",
    "print('The data is imbalanced, with {} samples in total but only {} cases of fraud'.format(len(labels), sum(labels)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-1.30935976e-01  1.18590069e-01 -2.27898637e-01  1.43754989e-01\n",
      " -8.40391596e-02 -4.11203755e-02 -1.80095560e-01  2.90122110e-02\n",
      " -8.48341442e-02 -1.80792781e-01  1.16248322e-01 -1.93028312e-01\n",
      " -1.21977554e-02 -2.15601424e-01 -3.67546134e-03 -1.27643149e-01\n",
      " -2.03370165e-01 -6.96279400e-02  1.31197147e-02  9.79558169e-03\n",
      "  1.57893112e-02  4.12297868e-03 -4.53991555e-03 -9.26844808e-03\n",
      "  3.63303940e-03  2.88372901e-03  7.25476920e-03  5.36939983e-03\n",
      "  8.65322987e+01]\n",
      "[5.81422266e+00 3.53893634e+00 5.22815921e+00 2.75932610e+00\n",
      " 2.99819153e+00 1.83731886e+00 3.91572881e+00 2.72613856e+00\n",
      " 1.53972757e+00 2.85541253e+00 1.66448352e+00 2.81317897e+00\n",
      " 9.96798873e-01 2.91861745e+00 8.52508745e-01 1.71793044e+00\n",
      " 3.55123593e+00 1.10269812e+00 7.02269898e-01 6.00834679e-01\n",
      " 9.70866692e-01 5.68914456e-01 4.62628746e-01 3.63621957e-01\n",
      " 2.86246268e-01 2.32051644e-01 1.99043866e-01 1.24127675e-01\n",
      " 5.05943367e+04]\n",
      "The PCA features have mean 0 but different variances\n",
      "The transaction amount is not scaled. We should scale it so it does not affect the SVM too much\n"
     ]
    }
   ],
   "source": [
    "print(np.mean(data, axis=0))\n",
    "print(np.var(data, axis=0))\n",
    "print('The PCA features have mean 0 but different variances')\n",
    "print('The transaction amount is not scaled. We should scale it so it does not affect the SVM too much')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 3.88442864e-16  2.15081172e-16 -1.81456682e-17 -3.38356153e-16\n",
      "  2.33030531e-16 -4.18546746e-18  9.87214729e-17 -2.56335807e-16\n",
      " -2.19097184e-16  1.29962098e-15 -1.07497991e-16 -3.43971347e-16\n",
      "  2.20525984e-16  5.92168641e-16  6.47658495e-16  1.64922234e-16\n",
      "  8.39908498e-17  3.92604256e-16 -2.34089862e-18 -1.08585101e-16\n",
      " -6.75712239e-17 -1.42913342e-16 -1.72819063e-16 -3.78497749e-17\n",
      " -8.33974764e-16  4.83106656e-17  1.47692831e-16 -2.01402472e-17\n",
      "  8.58712358e-15]\n",
      "[1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1.]\n"
     ]
    }
   ],
   "source": [
    "scaler = StandardScaler()\n",
    "data_sc = scaler.fit_transform(data)\n",
    "print(np.mean(data_sc, axis=0))\n",
    "print(np.var(data_sc, axis=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preliminary plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1.06937359  1.32270727 -0.24604595  1.15431212 -0.71476402  0.92706025\n",
      " -0.54962614 -0.42607169 -1.16572175]\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZUAAAEKCAYAAADaa8itAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMi4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvhp/UCwAAIABJREFUeJzt3X+cXHV97/HXZzeJYRN+ZQkYCNmNBfkdNWyRXtHSgojoDWKDJF0gopLLIhb13lvUeK+ojUW9LQYs0lAsgV1BmmKhimjKD8VS0AQh5adE8oMVNGFDAyE/THY/949zJpmdPWfmzMw582P3/Xw8zmNnvnPmzGcmk/OZ789j7o6IiEgaWuodgIiIjB5KKiIikholFRERSY2SioiIpEZJRUREUqOkIiIiqcksqZjZt81so5k9kVc2xcxWmNlz4d8Dw3Izs2vMbI2ZrTaz2XnPWRDu/5yZLcgrP9HM/jN8zjVmZlm9FxERSSbLmspNwJkFZZ8B7nX3I4F7w/sA7wWODLeFwLcgSELAF4C3AycBX8glonCfhXnPK3wtERGpscySirv/FNhcUHw2sCy8vQz4QF75zR54GDjAzKYB7wFWuPtmd38FWAGcGT62n7v/hwezN2/OO5aIiNTJuBq/3iHu/hKAu79kZgeH5YcBL+Tt1x+WFSvvjyiPZGYLCWo1TJo06cSjjz66yrchIjJ2rFq16mV3n5pk31onlThR/SFeQXkkd18KLAXo6urylStXVhKjiMiYZGbrk+5b69Ffvwubrgj/bgzL+4HD8/abDrxYonx6RLmIiNRRrZPKXUBuBNcC4M688gvDUWAnA1vCZrIfAWeY2YFhB/0ZwI/Cx14zs5PDUV8X5h1LRETqJLPmLzO7FTgVOMjM+glGcV0F3G5mHwU2AOeGu98NnAWsAbYBFwG4+2Yz+zLwi3C/L7l7rvO/h2CE2T7AD8NNRETqyMba0vfqUxGRJHbt2kV/fz87duyodyg1M3HiRKZPn8748eOHlZvZKnfvSnKMRumoFxFpKP39/ey77750dnYyFuZWuzsDAwP09/czc+bMio+jZVpERCLs2LGD9vb2MZFQAMyM9vb2qmtmSioiIjHGSkLJSeP9KqmIiEhqlFREREaByZMnA/Diiy8yd+7cyH1OPfVUsh6opKQiIjKKHHrooSxfvrxur6+kIiKSgr4+6OyElpbgb19fdce74ooruO666/bcv/LKK/niF7/IaaedxuzZsznhhBO4886Rc77XrVvH8ccfD8D27duZN28es2bN4rzzzmP79u3VBZWAhhSLiFSprw8WLoRt24L769cH9wG6uys75rx58/jkJz/JpZdeCsDtt9/OPffcw6c+9Sn2228/Xn75ZU4++WTmzJkT28H+rW99i7a2NlavXs3q1auZPXt25H5pUk1FRKRKixbtTSg527YF5ZV629vexsaNG3nxxRd5/PHHOfDAA5k2bRqf+9znmDVrFqeffjq/+c1v+N3vfhd7jJ/+9Kecf/75AMyaNYtZs2ZVHlBCqqmIiFRpw4byypOaO3cuy5cv57e//S3z5s2jr6+PTZs2sWrVKsaPH09nZ2fJeSW1HhatmoqISJVmzCivPKl58+Zx2223sXz5cubOncuWLVs4+OCDGT9+PPfffz/r1xdfkf5d73oXfWHnzhNPPMHq1aurCygBJRURkSotXgxtbcPL2tqC8mocd9xxvPbaaxx22GFMmzaN7u5uVq5cSVdXF319fZS64GBPTw9bt25l1qxZfO1rX+Okk06qLqAEtKCkiEiEp59+mmOOOSbx/n19QR/Khg1BDWXx4so76esp6n1rQUkRkRrr7m7OJJI2NX+JiEhqlFRERCQ1SioiIpIaJRUREUmNkoqIiKRGSUVEpEFdc801HHPMMXSnPKzsgQce4P3vf3+qx8zRkGIRkQZ13XXX8cMf/nDYNeN3797NuHGNe+pWTUVEJA0pr31/ySWX8PzzzzNnzhz2339/Fi5cyBlnnMGFF17IunXreOc738ns2bOZPXs2Dz30EDCyBnLZZZdx0003AXDPPfdw9NFHc8opp3DHHXdUFVsxjZvuRESaRQZr319//fXcc8893H///Xzzm9/kX//1X/nZz37GPvvsw7Zt21ixYgUTJ07kueeeY/78+UWv6Lhjxw4uvvhi7rvvPo444gjOO++8imJKQjUVEZFqZbH2fYE5c+awzz77ALBr1y4uvvhiTjjhBM4991yeeuqpos995plnmDlzJkceeSRmtmc5/CyopiIiUq2s1r7PM2nSpD23r776ag455BAef/xxhoaGmDhxIgDjxo1jaGhoz375y+LXagl81VRERKqV1dr3MbZs2cK0adNoaWnhlltuYXBwEICOjg6eeuopdu7cyZYtW7j33nsBOProo1m7di2//vWvAbj11lsziQuUVEREqpfV2vcxLr30UpYtW8bJJ5/Mr371qz21mMMPP5wPfehDzJo1i+7ubt72trcBMHHiRJYuXcr73vc+TjnlFDo6OjKJC7T0vYhIpHKXvh8ta99r6XsRkUagte8BNX+JiEiKlFRERGKMte6BNN6vkoqISISJEycyMDAwZhKLuzMwMLBneHKl1KciIhJh+vTp9Pf3s2nTpnqHUjMTJ05k+vTpVR1DSUVEJML48eOHLeQoyaj5S0REUlOXpGJmnzKzJ83sCTO71cwmmtlMM3vEzJ4zs++a2YRw3zeE99eEj3fmHeezYfmzZvaeerwXERHZq+ZJxcwOA/4C6HL344FWYB7wVeBqdz8SeAX4aPiUjwKvuPsRwNXhfpjZseHzjgPOBK4zs9ZavhcRERmuXs1f44B9zGwc0Aa8BPwpsDx8fBnwgfD22eF9wsdPs2BltLOB29x9p7uvBdYAJ9UofhERiVDzpOLuvwH+H7CBIJlsAVYB/+Xuu8Pd+oHDwtuHAS+Ez90d7t+eXx7xnGHMbKGZrTSzlWNpJIeISK3Vo/nrQIJaxkzgUGAS8N6IXXODw6PWa/Yi5SML3Ze6e5e7d02dOrX8oEVEJJF6NH+dDqx1903uvgu4A/hvwAFhcxjAdODF8HY/cDhA+Pj+wOb88ojniIhIHdQjqWwATjaztrBv5DTgKeB+YG64zwLgzvD2XeF9wsfv82CK613AvHB02EzgSODnNXoPIiISoeaTH939ETNbDjwK7AZ+CSwFfgDcZmZ/FZbdGD7lRuAWM1tDUEOZFx7nSTO7nSAh7QY+7u6DNX0zIiIyjK6nIiIiRZVzPRXNqBcRkdQoqYiISGqUVEREJDVKKiIikholFRERSY2SioiIpEZJRUREUqOkIiIiqVFSERGR1CipiIhIapRUREQkNSWTipn9OEmZiIhI7CrFZjYBmAgcYmb7sveiWPsBM2oQm4iINJliS99/HPg0cDDwJHuTyqvA9RnHJSIiTSg2qbj71cDVZvZJd/9GDWMSEZEmVfIiXe7+DTM7CejM39/dv5NhXCIi0oRKJhUzuwk4FngMyF1Z0QElFRERGSbJ5YRPBo5196GsgxERkeaWZJ7Kk8BBWQciIiLNL0lNZX/gaTN7GNiZK3T3D2YWlYiINKUkSeWvM49CRERGhSSjv+6tRSAiItL8koz+eo1gtFdu/1Zgp7vvl2VgIiLSfJLUVPbN3TazFuCDwFuyDEpERJpTWasUu/uQuy8H3p1RPCIi0sSSNH/NybvbAnSxdx0wERGRPZKM/jo37/ZuYB1wdibRiIhIU0vSp3JBLQIREZHml+QiXYea2T+Z2Uvh9l0zO7QWwYmISHNJ0lH/j8CPCVYp7gRWhGUiIiLDJEkqh7j7De6+M9z+ATgk68BERKT5JEkqm81snu11HrA568BERKT5JEkqHwEuBF4GNgEXAB/NMigREWlOSUZ/rQPOyj4UERFpdklGf80ws6+Z2e1mdkduq+ZFzewAM1tuZs+Y2dNm9kdmNsXMVpjZc+HfA8N9zcyuMbM1ZrbazGbnHWdBuP9zZragmphERKR6SSY/3gXcTDDqK62rPy4B7nH3uWY2AWgDPgfc6+5XmdlngM8AVwDvBY4Mt7cD3wLebmZTgC8QzPB3YJWZ3eXur6QUo4iIlClJUvm9u/9tWi9oZvsB7wI+DODuvwd+b2ZnA6eGuy0DHiBIKmcDN7u7Aw+HtZxp4b4r3H1zeNwVwJnArWnFKiIi5UmSVK41s88DP2L4lR9XV/iabyLo8P9HM3sLsAq4nGDo8kvhsV8ys4PD/Q8DXsh7fn9YFlc+gpktBBYCzJgxo8KwRUSklCRJ5c3AxwiaoXLNX05Q26j0NWcDn3D3R8xsCUFTV5yoxSu9SPnIQvelwFKArq6uyH1ERKR6SZLKh4BOd99Zcs9k+oF+d38kvL+cIKn8zsymhbWUacDGvP0Pz3v+dODFsPzUgvIHUopRREQqkGSeympg35J7JeTuvwVeMLOjwqLTgKcIBgTkRnAtAO4Mb98FXBiOAjsZ2BI2k/0IOMPMDgxHip0RlomISJ0kqam0A8+Y2SMM71P5YBWv+wmgLxz59TxwEUGCu93MPgpsYO+S+3cTzJNZA2wL98XdN5vZl4FfhPt9KddpLyIi9WHBoKoiO5idFlXu7vdmElHGurq6fOXKlfUOQ0SkaZjZKnfvSrJvkhn1TZk8RESk9pLMqP9DM3vYzLaY2Q4z22lmr9YiOBERaS5J+lSuA84HbgNOIpi0eHixJ4iIyNiUZPRXi7s/C4xz913ufgNwesZxiYhIE0pSU3k9HKX1uJl9BXgJmJxtWCIi0oyS1FQ+HO53GTBIsLDj3AxjEhGRJpVk9Nfz4c0dwP/JNhwREWlmSWoqIiIiiSipiIhIapRUREQkNSX7VMzsCODTQGf+/u5+RnZhiYhIM0oypHg5cCPQSzD6S0REJFKSpDLk7tdmHomIiDS9JH0qd5rZQjObamb75bbMIxMRkaaTpKbysfBv/hwVB3SxdxERGaZkTcXdD4/YlFBEJLG+PujshJaW4G9fX70jkqwkWfp+nJldama3hdslZpakhiMio1ySZNHXBwsXwvr14B78XbhQiWW0SnLlx78HJgE3h0XnAzvcfWHGsWVCV34USUcuWWzbtresrQ2WLoXu7r1lnZ1BIinU0QHr1mUdpaShnCs/Jkkqj7v7W0qVNQslFZF0JE0WLS1BDaWQGQwNZRWdpKmcpJJk9NeQmXXmHbwT0FdBZIzbsCFZ+YyYHti4cmluSZLKXwI/NbN/M7N7gZ8A/zvbsESk0SVNFosXB81i+dragvJi1LnfnJKM/loBHEWQXP4SONrd/y3rwETqRSezZJImi+7uoJ+loyNo8uroGNnvUkid+80rtk/FzP7Y3X9iZnOiHnf3uzKNLCPqU5FiknY+S6CvDxYtCpq8ZswIEkoan5M69xtLKh31ZvZX7v55M7sl4mF39wurCbJelFSkGJ3MGoM69xtLOUkldr6Ju38+vLnI3Yd1vZmZuthkVEra+SzZmjEjOrmrc7/xJemo/5eEZSJNTyOVslFOP1VfH2zdOrI8See+1F9sTcXM3gwcA+xf0K+yHzAx68BE6mHx4ug+FZ3MKlfYT5XrdIeR/S9RfVoA7e2wZIn6tZpBsT6Vc4APAmcBd+c99Bpwq7s/mH146VOfipSSVefzWFVOP5X6tBpT2jPqT3H3n6USWQNQUhGprXI63dVB35jSnlF/kZkdkHfwA83shoqjE5ExpZx+KvVpNb8kSWW2u/9X7o67vwKcmF1IIjKalDOjvtLZ99I4kiSVFjPbP3fHzA4ExmcXkoiMJuXMqK9k9r00liR9KhcRrPX1XYIrPs4DvubuN2UeXQbUpyIytmjgRfVSmfyY4+7/aGaPAn8CGHCeu/9nlTGKiGSunOHMko4kzV+4++MEF+n6LjBgZodmGpWISAlJJlQuWjRyzsu2bUG5ZCPJ5YTfZ2a/AvqBh4EXgPuqfWEzazWzX5rZ98P7M83sETN7zsy+a2YTwvI3hPfXhI935h3js2H5s2b2nmpjEpHmkHQVYy27U3tJaiqLgXcAz7r7DOBM4IEUXvty4Om8+18Frnb3I4FXgI+G5R8FXnH3I4Crw/0ws2MJ+neOC2O6zsxaU4hLRBpcXA3k8suHl2mIcu0lSSq73X0TwSgwC6+vMruaFzWz6cD7gH8I7xvwp8DycJdlwAfC22eH9wkfPy3c/2zgNnff6e5rgTXASdXEJSLNIa6mMTAwvLaiIcq1lySpbDGzScDPgJvN7G+o/nLC3yC44FfuOO3Af7n77vB+P3BYePswgiY3wse3hPvvKY94zjBmttDMVprZyk2bNlUZuojUW7GaRn5/iYYo116SpPIBYAfwSYJmr98A/73SFzSz9wMb3X1VfnHErl7isWLPGV7ovtTdu9y9a+rUqWXFKyKNp1hNo7AW090drBs2NBT8VULJVqLmL2DI3XcRJJVnCPo8KvUOYI6ZrQNuI2j2+gZwgJnlhjhPB14Mb/cDhwOEj+8PbM4vj3iOiIxi3d3BysVR1F9SX0mSyoPAPmY2DfgJcAnw7Upf0N0/6+7T3b2ToKP9PnfvBu4H5oa7LQDuDG/fFd4nfPw+D2Zs3gXMC0eHzQSOBH5eaVwi0lyWLEnWX1LOtVykeomWaXH3bcCfAd909znArAxiuQL4tJmtIegzuTEsvxFoD8s/DXwGwN2fBG4HngLuAT7u7oMZxCUiDShJf0nSoceSniTLtDwGXAwsARa6+xNm9p/ufkItAkyblmkRGTt0fZZ0pL30/aeBLwI/CBPKmwiaxEREGpomP9ZekrW/7iNvBr27Pw9cmmVQIiJpmDEjuqaizvzsJFmm5Qgzu87M7jazH+e2WgQnIlINTX6svZI1FYJZ7DcCvYA6wkWkaeQ67bX0fe0k6VMZcvdr3f0hd38kt2UemYhICnKTH2+5Jbh/wQUaWpylJDWVO81sIfA9YGeu0N1fzSwqEZEU6boqtZNkSPELEcUerljcdDSkWGTsOeigYLHJQhpanEyqQ4rd/fCIrSkTioiMPX190QkF0hlarBn7wyVp/sLMjgaOBSbmytz9O1kFJSKSlmJXeax2aLGa1UZKMqT488BS4HrgvQSLP84t+iQRkQZRrDZS7dBiXa54pCSjv84D/gR4yd0vAN5CwhqOiEi9xdVG2turr01oxv5ISZLK9nChxt1mti/wW+BN2YYlIpKOuAmQS5ZUf2xdrnikJEnll2Z2AMFy9ysJlpd/NNOoRETKFNdhnuXVHzVjf6SiQ4rDa8G/0d1fCu8fAezn7k2bVDSkWGT0Kewwh+DkXotLB/f1jf4Z++UMKU4yT2WVu5+YSmQNQElFZPTREvfZSnvp+5+b2ewqYxIRyYw6zBtHbFLJu178KQSJ5Vkze9TMfmlmTdv8JSKjjzrMG0exmkrueu8fAI4CzgLOJZijcm7GcYmIJBbVYQ6wdatmuNdasfkmBuDuv65RLCIiFcl1jF9++fAlWQYGNMO91mI76s2sH/jbuCe6e+xjjUwd9SKjlzrss5FWR30rMBnYN2YTEWkoWXTYa8HI8hRLKi+5+5fc/YtRW80iFJExqZKTeVzH/JQplSWG3PyX9evBPfh7wQXBJEolmGjFkorVLAoRkTxRJ/OFC0ufxKM67MePh9deiz5WqcQVtWBkrscgaUxjTbE+lSnuvrnG8WROfSoija+avpHCGe5bt0ZfT6W9HbZvLz4Lv6VlbxKJMxb6a1LpUxmNCUVEmkM1fSO5a9IPDQV/N8ecyQYGopetP//8vbWWJPNc1q9XbSVfkhn1IiI1leZkxkqek2vaOuus6PkvhdQMtpeSiog0nDRX/407Vnt78edt2wZ33713hWMIOujj9h3LF+bKp6QiIg0nzeXq4461ZEnpWsiGDXub09zhllvi912/XiPCIMEqxaONOupFJCfXqR81KACiO+HjBhHk1GrJ/VpKe5ViEZFRKVcL6e1N3twWt85YzrZtsGDB2K2xKKmIyJiX30QG0Nq6t5+kMDkU7htlcHDsdt4rqYiIECSLXC1kcDAoi5souWhRsG+xxDJWO++VVEREQpdfHj135fLLo2f45w85nk8fa+lkkBbW0sl8+sbkRcKUVEYRLXwnUrm+vuiZ9xA/UTI35Ljb+riBhXSynhacTtZzAwu5bMrY+09Y86RiZoeb2f1m9rSZPWlml4flU8xshZk9F/49MCw3M7vGzNaY2er8Sxub2YJw/+fMbEGt30sjqXStJJGxLP+H2IIKziC5IcfXT1nEJIZnnUls438OLBpzi0/WfEixmU0Dprn7o2a2L7CK4OqSHwY2u/tVZvYZ4EB3v8LMzgI+QXDlybcDS9z97WY2BVgJdAEeHudEd3+l2OuP1iHFuo6ESHlyP8QKayBR2tujazF7/n/FLBI2hNHKENDcQ40bekixu7/k7o+Gt18DngYOA84GloW7LSNINITlN3vgYeCAMDG9B1jh7pvDRLICOLOGb6WhZHEdCZHRLGoF4ijt7dETJc32TnjcOiV6LZgN7C3P9c2MdnXtUzGzTuBtwCPAIe7+EgSJBzg43O0w4IW8p/WHZXHlUa+z0MxWmtnKTZs2pfkWGkaaayWJjBbF+hmT/OBqawsSSuEwYrPhS+B/4tXF7J4wPOu8ThufY/hEl4GB0d8MVrekYmaTgX8GPunurxbbNaLMi5SPLHRf6u5d7t41derU8oNtAmmulSQyGpTqZyz1g+sT7X38bp9Oui8IMlI3faxbFySWwpaum3Z1c+Hvl7KODoYw1tHBxSzlVka2dY36YcbuXvMNGA/8CPh0XtmzBH0tANOAZ8Pbfw/ML9wPmA/8fV75sP3ithNPPNFHq95e944Od7Pgb29vvSMSqZ+ODvfg9D986+gIHu/tDf6vRO3Tbb2+a0Lb8MK2Nvfe3tjnJN3M6vmpVAZY6QnP7/XoqDeCPpPN7v7JvPKvAwO+t6N+irv/pZm9D7iMvR3117j7SWFH/SogNxrsUYKO+qLXgRmtHfUiMlzcBbbMgmutAFx6KVx//cj91tJJJyNHvvS3dnD44Lqq4mpvh5dfruoQNdfQHfXAO4ALgD81s8fC7SzgKuDdZvYc8O7wPsDdwPPAGuAG4FLYcxGxLwO/CLcvlUoo0lg0r0aylKSf8brrgpWHW1sL9iG6w+XQwepHvrz66ij/riet0oyWbTQ3fzWT3t6gNSGidUGaXLnNsFk125bzHSts0lpLR2Tb1Vo6qmr6imqKa4bvPGU0f9X9JF/rTUmlMZRq75bmVO6Phax/XCRNWIXfx/n0+laGB7aVNp9Pb6pJpVl+TCmpKKk0vLjOzmbsxMxCsw66KPfHQqP8uIhKbvPp9bV0+CDma+nIJKE0y4+pcpKK1v6SutC8mnjNvOROuZNwG2XSbm4eSn7fyq10M5N1tDLETNZFDg8uJmqByTjr1zfHv28SSipSF5pXEy9qpnezLKNe7o+FRvpx0d0Ny5aVvsRwEvOJXmCyWGJplh8OpSipSF2keQ3y0aZRfr1XotwfC5n+uKhgeGHU97ISXyF6gcmvEP/LoFl+OJSUtJ1stGzqU8les/YHNIpG6WeoVEOM/kpxBED+v0dUP0tU2SDRnYaDWNG+lUbtU0Qd9Uoq9aKhwtXTZ5iCFDNzb6/7hAnRI8J2MMG3M35Y2VbafCPtka8/BIk6/dvbG+vfu5ykUvMZ9fWmGfXZ0hL86ejrC5pCNmwI+hcWL1bTYFmSTKcvw+TJ8MTr0bPso2yinTa2j2gCy3mdtti1wXImTIBvf7sx/t0bfUZ909IM8NKauT+gkXR3B0l4aCj4m+qJZSx8kVMcAdDXB6+/Hj/LPko7m7mYYIHJqJ/tpfpXAH7/++ZcKl9JJaFmHuZZS400mkcijJUvcoojAHKd5/nXRillAzP2DEn2yAXVkyWpgQE4/fTEL9sYkraTjZat0j6VZu88rRX1BzS4Cr/ITTn4IqWgcxN1y+lTye8zSWPJl56eND+Y8qGO+vSTimaAJ9eUJ6CxooIvcr1/KNT7+1TJ6K/8z6qcJV+KHaue/4+UVDJIKqqpyKhQwRe5nt/9eie0XAzjx0d/Bkm3JEu+lEo+48fXL7EoqWSQVBrhyy1StQq+yPWspdcioeXXhNrbg62wVjRpUjaJJH9L0kw2YUJ677scSioZJBX3+lfDRVJR5he5njWVrBNaVI4tzLc9PZUllHJXOU46YbIe/StKKhklFZGxqJ619HITWrk//OKOn7+1tpafVOJqHRtpj629JO3Qb2lJ8QNOqJykoiHFIlJUvdZp6+uDrVtHlheODM5NuzGDCy4ob7R0kvlTg4Nlhx47XPggBmIXmfwci3md4cOgX6eNzzF8GPTQUIOPAE+afUbLppqKSHVq0Qwc1yxVuHxJqearUs10SWoqSZq6NtLuQwTLsGykPXaZllI1kaT9MK2t6X/mxaDmLyUVkSwUawpLM9kkbfZKkhT+nPjAkiSlUgllBxNGPLCTlsjyqK3UIpNx26GHVv75lktJRUlFJBNxJ/H29gT9LmVknWId9PmHSXLSf92KB9bbm7zfpLBWMlhk59w+pQ66kfair1es5lKrxKKkoqQikokkJ/LImkWZvf3lJK9i23qLOVBelaenJ3mCSlr7KGeLSypJR5DVYjSYkkqRTUlFpHLl9kHsGfpb5jCuuBzUnqCrIpcgOjrch2KG6eaqPC9Prn4eSbVbXPNXOcu7ZL1UfjlJRaO/pObGwiK5o1XcOo3t7dH771lItMzlq+NGnG3eHB9bbr9bbglOtevWgXVELwI56MaO8y+ifWvyy/2Ws0pxOeIWqox7vQ7Wj7jm/cAAfOQjDfJ/KWn2GS2bair1pZUJGlM5nexR+5b8d01pBmXZhymzJ77YIo/ljOgq3OL6VnKjxaJqSaVqRlFNYVlNSEXNX0oqjUprqDWetBJ9XLLp6AhGYJXqME9y/KjmrwkT4g/T2+v+ifZe30Wynvi4pqj59I5YjTjNbRDzQYZfFTKqTyVJEsziB5qSSpFNSaW+tNpz40k70ecSCQTJJDd6aSPtvon2oJ+jzDHHxSoccQst5i4DDPFLoBRuu2gta7Z7ktFd5W47mDAssaylI/Z1opLguHHpJxYllSKbkkp9qabSeNJK9IXMAj9mAAANDklEQVQ1ibjRSxdP6t2zf9Imt1IDBFpb9x6np2fk/kk62QtP3PnNS3FDhwfDE3vaiaVwRFi512RJ+0eakkqRTUmlvsZkn0qDr0SaRqKP+nctdiLs6Snve1DuUObCLSrBDeVtu2mJjXU+vUUWe0wWQLk1mqHwtXO1pmvpGRH/dsb7RtpjR6+ddloKX46QkkqRTUml/pKeYxv8XJxME2TRNEKMSkzFVt1tbY2e2BeXyIrVVObT61uYNCxJDIJfS8+I/YLXG1m7KNaRXiwhZNH8FXXcrbT5tfQMa0osnDMT1XGfFiWVIpuSSnNognNxMpVWA2qcUUu9XKnHo2oSxWoqcU1jf070+8z1j1xLj++mZc/J/nUm+K6Y5DUUJpZr6fFdtPoQQZ/JFiq4OEoGSaPchJTf1BX32e6idVhiSWtFYyWVIpuSSm1Vem4cNX0vlXRYNFhGTRJO1L9XsRnh62JOii+0dkS+fnu7+z2cVlEzUuFzitVK6p1sisWR3ylfrB+nsMaSxlIuSipFtkqSyqhohqmDas6NjTJKrNi/faLvRYLs2NOzd+2p1lb3lyeXfk4l8RbdoaD8wZ7ePXdzsRX+4r9hQs+wkV5R/2ZRTVxm8TPdh3Lvs3dvZ35bW3CcWp30651c4l4/SU0lal9wP/bYkl+dopRUimzlJpUG+9HYVKqpbdSlphJxYi22Im+i70WJHaOuKhjb+ZuXUSuagBi3Q0SveeGv3WvpifzFXzi3IpdYSi2nUnI414QJvn1Suw/CnkRWixN3o265GtcWJg3rPyq2f2HnfTVrhCmpFNnKTSo1Pbkl/BVZdJ3xtMpTiK1wVEr+r9ZSx3iwp9fX2/BjtLW5P9iT0WeUP6kh3HaG8wUKf21/or03trnnhdaI1yusivT0FP2M4po21tKxZ8hsYW6YMCFoPy98Wq52UfSkFbNEb27ORu4kVuwYcZP3olbYvWlSsIJjvU/slfRr1CPGap5f+OOg0sSipFJkKzep1KwZpoxfkT5hQjDjK8m+lZRH9dJWEVtu5MqI2cFlHON1a/OnTyvjfZT7GU2eHPkPvYXJkUNRC5fWiJz9XOZnlDsJx51cCmsDhVv+SXwLk30whZNSpSeyqH/vrbT5ioh+kUY/sTf7thsb9l2tpJWlnKRiwf7Ny8zOBJYArcA/uPtVxfbv6urylStXJj5+Z2dwWdJCHR3BgnWpiXuh1tbk1zM1C743ScUdu6MDJk2Cp55K7fUGMVqJ2Lec9xennGOU+XoOWJHHhmihnzcygxdj90vqVSazL1sjjxO8lmE4AwSrN7YzwBCttDLIEEYLXnUMadlNK+MY+TkX+zwlO7toZQHLuJXgGtDlnvbNbJW7dyXZt6lXKTazVuDvgPcCxwLzzezYNF8jblXWxYuj969Y3Cqu5Zxwy/2mxB17/frSCaXM12uJSijFYihHOcdI4/VCBrQylEpCAWITyt7XCpYVn8oAUxmgBRjH4J7Hqoqh8EtepdaIhALFE7RkZzyDLOHyPfezXM24qZMKcBKwxt2fd/ffA7cBZ6f5AnFLcHd3p/kq5K0RXqC1NeUXqtGxC8Se8NKIoZxjlPl6SU7Vaf3yrtsv+NyXOsXvwyDlHUu1l+wdxMCe24sWZfc6Td38ZWZzgTPd/WPh/QuAt7v7ZQX7LQQWhnePAp7Ne/gg4OUahFvUQTBlBnRYXqJ3GNoMA1Og/WVombq33AGsiv+L+ccufE2r4sfGJvCDyohtADZFxJD4/cW9j1L75n+eRfb3zfBykmNnZRNQKs5qOAxtgPUvw+YT4cRKj1MY50amchADtDBUfZApyvrzTEtWca4a9k+8alUZT+1w90QhjSsvpIYT1/w8vMB9KbA08gBmK5O2FdaTma1crzhTozjTpTjT1SxxRmn25q9+4PC8+9OBF+sUi4jImNfsSeUXwJFmNtPMJgDzgLvqHJOIyJjV1M1f7r7bzC4DfkQwpPjb7v5kmYeJbBZrQIozXYozXYozXc0S5whN3VEvIiKNpdmbv0REpIEoqYiISGrGZFIxsy+b2Woze8zMfmxmh4blp5rZlrD8MTP7vw0ap5nZNWa2Jnx8dp3j/LqZPRPG8j0zOyAs7zSz7Xmf5/X1jLNYrOFjnw0/02fN7D11jvNcM3vSzIbMrCuvvKE+07g4w8ca5vPMZ2ZXmtlv8j7Ds+odUz4zOzP8zNaY2WfqHU/Zki4SNpo2YL+8238BXB/ePhX4fr3jSxDnWcAPCebpnAw8Uuc4zwDGhbe/Cnw1vN0JPFHvzzFhrMcCjwNvAGYCvwZa6xjnMQQTdR8AuvLKG+ozLRJnQ32eBTFfCfyvescRE1tr+Fm9CZgQfobH1juucrYxWVNx91fz7k6iQZceKhLn2cDNHngYOMDMptU8wJC7/9jdd4d3HyaYL9SQisR6NnCbu+9097XAGoJlgOrC3Z9292dL71lfReJsqM+ziWS+9FTWxmRSATCzxWb2AtAN5Ddz/ZGZPW5mPzSz4+oU3h4xcR4GvJC3W39Y1gg+QlCLyplpZr80s5+Y2TvrFVSM/Fgb+TMt1MifaU6jf56XhU2g3zazA+sdTJ5G/9xKaup5KsWY2b8Bb4x4aJG73+nui4BFZvZZ4DLgC8CjBGvcbA3bWf8FOLIB40y0PE2aSsUZ7rMI2A3k1kB9CZjh7gNmdiLwL2Z2XEENrFFibcjPNELNP9MK46z55znsxYvEDHwL+HIYz5eBvyH4gdEI6vq5pWHUJhV3Pz3hrt8BfgB8If8/prvfbWbXmdlB7p7ZgpOVxEkdlqcpFaeZLQDeD5zmYeOwu+8Edoa3V5nZr4E3A8kvaFOjWGnAzzTmOTX/TCuJkzovoZQ0ZjO7Afh+xuGUo+mXnhqTzV9mll/7mAM8E5a/0cwsvH0SweczMPIItREXJ8FSNBeGo8BOBra4+0s1DzBkwYXSrgDmuPu2vPKpFlzzBjN7E0Gt7/n6RLknpshYCT7TeWb2BjObSRDrz+sRYzGN+JnGaNjPs6D/8RzgiXrFEqHpl54atTWVEq4ys6OAIWA9cElYPhfoMbPdwHZgXt4v2XqIi/NughFga4BtwEX1CW+PbxKM8lkR5uSH3f0S4F3Al8LPcxC4xN031y9MICZWd3/SzG4HniJoFvu4u6d3Ra8ymdk5wLUEK6D/wMwec/f30GCfaVycjfZ5Fviamb2VoFlpHfA/6hvOXp7O0lN1pWVaREQkNWOy+UtERLKhpCIiIqlRUhERkdQoqYiISGqUVEREJDVKKjImmNlg3qq0j4Wr/XaZ2TVlHOMAM7s0wWs8YWb/ZGZtYfkbzew2M/u1mT1lZneb2ZvznvcpM9thZvsXOfbXw9WAv5403rznvrXRVuKV0UtDimVMMLOt7j454b7j8hadzC/vJFjF+vhSr2FmfcAq4GrgIWCZu18fPvZWYF93fzC8/3OCWfI3uvtNMcd+FZgazqgvi5l9mGAF4cvKeI4RnB+Gyn09GdtUU5Exy4Lr53w/vH2lmS01sx8DN5vZcWb287DmsTpc3eAq4A/CslI1hgeBI4A/AXblEgqAuz+Wl1D+AJgMfB6YHxPnXQSrVD9iZueFs+r/2cx+EW7vCPc7ycwesmCxyYfM7KhwVvaXgPPCuM8L3+v/yjv+E2HNrdPMnjaz6wjWwTvczM4ws/8ws0fD2leixCxj11idUS9jzz5m9lh4e627nxOxz4nAKe6+3cyuBZa4e194Ym4FPgMc7+5vLfZCZjYOeC9wD3A8QY0lznzgVoIkdJSZHezuG/N3cPc5YS3oreHxvwNc7e4/M7MZBLOvjyFYxudd4azs04GvuPufWXCxuT01FTO7skg8RwEXufulZnYQQbI73d1fN7MrgE8TJCmRSEoqMlZsL5UMgLvcfXt4+z8IVoeeDtzh7s+Fy7oUk5+4HgRuZO/SOnHmAee4+5CZ3QGcC/xdieecDhybF89+ZrYvsD+wLKxVOTC+VMAR1ofX6IHgAnDHAv8evtYEgs9FJJaSisher+duuPt3zOwR4H3Aj8zsY5ReuHFE4jKzJwnWlBvBzGYRLLS4Iu+k/Tylk0oL8Ed5CTB3vGuB+939nLD/54GY5+9meNP3xLzbr+fdNmCFu0c2y4lEUZ+KSAQLVgB+3t2vIVgldhbwGrBvmYe6D3iDmV2cd+w/NLM/Jmj6utLdO8PtUOAwM+soccwfE1xbJ3e8XCLbH/hNePvDefsXxr0OmB0+dzbB5X6jPAy8w8yOCPdtyx+1JhJFSUUk2nnAE2Fz1tEEl28eIGgKeiLp0N5wletzgHeHQ4qfJLhG+osETV/fK3jK98LyYv4C6AoHEDzF3ia2rwF/bWb/TtAHlHM/QXPZY2Z2HvDPwJTwvfUAv4qJfRNBcrrVzFYTJJmjS79rGcs0pFhERFKjmoqIiKRGSUVERFKjpCIiIqlRUhERkdQoqYiISGqUVEREJDVKKiIikpr/D/mZ+5od82HVAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot the first two principal components\n",
    "data_fraud = data[np.where(labels == 1)]\n",
    "data_valid = data[np.where(labels == 0)]\n",
    "plt.scatter(data_valid[:, 0], data_valid[:, -1], c='b')\n",
    "plt.scatter(data_fraud[:, 0], data_fraud[:, -1], c='r')\n",
    "print(data_valid[1:10, 0])\n",
    "plt.ylim((0, 10000))\n",
    "plt.legend(['valid', 'fraud'])\n",
    "plt.xlabel('First PCA feature')\n",
    "plt.ylabel('Transaction amount')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting up cross-validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "fraud_indices = np.where(labels == 1)[0]\n",
    "np.random.shuffle(fraud_indices)\n",
    "fraud_splits = np.array_split(fraud_indices, 5)\n",
    "\n",
    "valid_indices = np.where(labels == 0)[0]\n",
    "np.random.shuffle(valid_indices)\n",
    "valid_splits = np.array_split(valid_indices, 5)\n",
    "\n",
    "folds = [np.concatenate((fraud_sp, valid_sp)) for fraud_sp, valid_sp in zip(fraud_splits, valid_splits)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_binary_SVM(data, scale=False, kernel=None, class_weight=None):\n",
    "    confusion_mat = np.zeros((2, 2))\n",
    "    for i in range(5):\n",
    "        if kernel is None:\n",
    "            if class_weight is None:\n",
    "                svm = LinearSVC(fit_intercept=False, dual=False)\n",
    "            else:\n",
    "                svm = LinearSVC(fit_intercept=False, dual=False, class_weight=class_weight)\n",
    "        else:\n",
    "            if class_weight is None:\n",
    "                svm = SVC(kernel=kernel, gamma='auto')\n",
    "            else:\n",
    "                svm = SVC(kernel=kernel, gamma='auto', class_weight=class_weight)\n",
    "        \n",
    "        train_data = np.delete(data, folds[i], axis=0)\n",
    "        test_data = data[folds[i]]\n",
    "        if scale:\n",
    "            scaler = StandardScaler()\n",
    "            train_data = scaler.fit_transform(train_data)\n",
    "            test_data = scaler.transform(test_data)\n",
    "\n",
    "        svm.fit(train_data, np.delete(labels, folds[i]))\n",
    "        pred = svm.predict(test_data)\n",
    "\n",
    "        conf_mat = confusion_matrix(labels[folds[i]], pred)\n",
    "        print('Fold', i)\n",
    "        print(conf_mat)\n",
    "\n",
    "        confusion_mat += conf_mat\n",
    "\n",
    "    confusion_mat /= 5\n",
    "    print('Final Confusion Matrix')\n",
    "    print(confusion_mat)\n",
    "    print('False negatives (valid):', confusion_mat[0, 1] / sum(confusion_mat[0, :]))\n",
    "    print('False positives (fraud):', confusion_mat[1, 0] / sum(confusion_mat[1,:]))\n",
    "    fscore = 2*confusion_mat[1,1] / (2*confusion_mat[1,1] + confusion_mat[0,1] + confusion_mat[1,0])\n",
    "    print('F-score: {}'.format(fscore))\n",
    "    return confusion_mat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Linear SVM -- Without scaling or class weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 0\n",
      "[[2894    5]\n",
      " [  20   79]]\n",
      "Fold 1\n",
      "[[2889   10]\n",
      " [  18   81]]\n",
      "Fold 2\n",
      "[[2888   11]\n",
      " [  28   70]]\n",
      "Fold 3\n",
      "[[2888   11]\n",
      " [  19   79]]\n",
      "Fold 4\n",
      "[[2889   10]\n",
      " [  16   82]]\n",
      "Final Confusion Matrix\n",
      "[[2889.6    9.4]\n",
      " [  20.2   78.2]]\n",
      "False negatives (valid): 0.0032424974129010003\n",
      "False positives (fraud): 0.20528455284552843\n",
      "F-score: 0.8408602150537635\n"
     ]
    }
   ],
   "source": [
    "train_binary_SVM(data);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Linear SVM with Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 0\n",
      "[[2899    0]\n",
      " [  16   83]]\n",
      "Fold 1\n",
      "[[2899    0]\n",
      " [  16   83]]\n",
      "Fold 2\n",
      "[[2898    1]\n",
      " [  29   69]]\n",
      "Fold 3\n",
      "[[2899    0]\n",
      " [  19   79]]\n",
      "Fold 4\n",
      "[[2897    2]\n",
      " [  18   80]]\n",
      "Final Confusion Matrix\n",
      "[[2.8984e+03 6.0000e-01]\n",
      " [1.9600e+01 7.8800e+01]]\n",
      "False negatives (valid): 0.00020696791997240426\n",
      "False positives (fraud): 0.1991869918699187\n",
      "F-score: 0.8863892013498313\n"
     ]
    }
   ],
   "source": [
    "train_binary_SVM(data, scale=True);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Linear SVM with Scaling and Class Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 0\n",
      "[[2824   75]\n",
      " [  10   89]]\n",
      "Fold 1\n",
      "[[2814   85]\n",
      " [  10   89]]\n",
      "Fold 2\n",
      "[[2837   62]\n",
      " [  12   86]]\n",
      "Fold 3\n",
      "[[2828   71]\n",
      " [   5   93]]\n",
      "Fold 4\n",
      "[[2809   90]\n",
      " [   7   91]]\n",
      "Final Confusion Matrix\n",
      "[[2822.4   76.6]\n",
      " [   8.8   89.6]]\n",
      "False negatives (valid): 0.026422904449810276\n",
      "False positives (fraud): 0.0894308943089431\n",
      "F-score: 0.6772486772486773\n"
     ]
    }
   ],
   "source": [
    "train_binary_SVM(data, scale=True, class_weight='balanced');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Kernel SVM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### RBF Kernel with Scaling and Class Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 0\n",
      "[[2899    0]\n",
      " [  22   77]]\n",
      "Fold 1\n",
      "[[2899    0]\n",
      " [  22   77]]\n",
      "Fold 2\n",
      "[[2898    1]\n",
      " [  30   68]]\n",
      "Fold 3\n",
      "[[2899    0]\n",
      " [  19   79]]\n",
      "Fold 4\n",
      "[[2897    2]\n",
      " [  15   83]]\n",
      "Final Confusion Matrix\n",
      "[[2.8984e+03 6.0000e-01]\n",
      " [2.1600e+01 7.6800e+01]]\n",
      "False negatives (valid): 0.00020696791997240426\n",
      "False positives (fraud): 0.21951219512195122\n",
      "F-score: 0.8737201365187713\n"
     ]
    }
   ],
   "source": [
    "train_binary_SVM(data, scale=True, kernel='rbf');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 0\n",
      "[[2890    9]\n",
      " [  45   54]]\n",
      "Fold 1\n",
      "[[2889   10]\n",
      " [  50   49]]\n",
      "Fold 2\n",
      "[[2892    7]\n",
      " [  50   48]]\n",
      "Fold 3\n",
      "[[2889   10]\n",
      " [  48   50]]\n",
      "Fold 4\n",
      "[[2890    9]\n",
      " [  42   56]]\n",
      "Final Confusion Matrix\n",
      "[[2890.     9. ]\n",
      " [  47.    51.4]]\n",
      "False negatives (valid): 0.003104518799586064\n",
      "False positives (fraud): 0.4776422764227642\n",
      "F-score: 0.6473551637279596\n"
     ]
    }
   ],
   "source": [
    "train_binary_SVM(data, kernel='rbf', class_weight='balanced');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Polynomial Kernel with Scaling and Class Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 0\n",
      "[[2897    2]\n",
      " [  16   83]]\n",
      "Fold 1\n",
      "[[2896    3]\n",
      " [  16   83]]\n",
      "Fold 2\n",
      "[[2897    2]\n",
      " [  26   72]]\n",
      "Fold 3\n",
      "[[2894    5]\n",
      " [  16   82]]\n",
      "Fold 4\n",
      "[[2895    4]\n",
      " [  18   80]]\n",
      "Final Confusion Matrix\n",
      "[[2895.8    3.2]\n",
      " [  18.4   80. ]]\n",
      "False negatives (valid): 0.0011038289065194895\n",
      "False positives (fraud): 0.18699186991869915\n",
      "F-score: 0.881057268722467\n"
     ]
    }
   ],
   "source": [
    "train_binary_SVM(data, scale=True, kernel='poly');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 0\n",
      "[[2877   22]\n",
      " [  13   86]]\n",
      "Fold 1\n",
      "[[2868   31]\n",
      " [  15   84]]\n",
      "Fold 2\n",
      "[[2884   15]\n",
      " [  18   80]]\n",
      "Fold 3\n",
      "[[2876   23]\n",
      " [  10   88]]\n",
      "Fold 4\n",
      "[[2878   21]\n",
      " [  16   82]]\n",
      "Final Confusion Matrix\n",
      "[[2876.6   22.4]\n",
      " [  14.4   84. ]]\n",
      "False negatives (valid): 0.007726802345636426\n",
      "False positives (fraud): 0.14634146341463414\n",
      "F-score: 0.8203125\n"
     ]
    }
   ],
   "source": [
    "train_binary_SVM(data, scale=True, kernel='poly', class_weight='balanced');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interestingly, balanced weights peform worse than only using scaled data. The balanced weights overcompensate too much.\n",
    "Simply scaling the data works best, with the poly kernel (and pretty well with RBF and linear)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_logistic_regression(data, folds, labels, class_weight = None, C=1.0):\n",
    "    err = 0\n",
    "    err_valid = 0\n",
    "    err_fraud = 0\n",
    "    confusion_mat = np.zeros((2, 2))\n",
    "    AU = 0\n",
    "    for i in range(5):\n",
    "        model = LogisticRegression(class_weight=class_weight, C=C)\n",
    "        \n",
    "        model.fit(np.delete(data, folds[i], axis=0), np.delete(labels, folds[i]))\n",
    "        pred = model.predict(data[folds[i]])\n",
    "        pred1 = model.predict_proba(data[folds[i]])[:,1]\n",
    "        conf_mat = confusion_matrix(labels[folds[i]], pred)\n",
    "        fpr, tpr, thresholds = roc_curve(labels[folds[i]], pred1)\n",
    "        area_under_curve = auc(fpr, tpr)\n",
    "        AU += area_under_curve\n",
    "        #print('Fold', i)\n",
    "        #print(conf_mat)\n",
    "        #print('Area under curve:', area_under_curve)\n",
    "\n",
    "        confusion_mat += conf_mat\n",
    "    AU/=5\n",
    "    confusion_mat /= 5\n",
    "    print('Final Confusion Matrix')\n",
    "    print(confusion_mat)\n",
    "    return confusion_mat, AU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Logistic Regression with Scaling, Class Weights, and Regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking class weight None and coefficient 0.01\n",
      "Final Confusion Matrix\n",
      "[[2.8984e+03 6.0000e-01]\n",
      " [1.8200e+01 8.0200e+01]]\n",
      "F1 score: 0.8950892857142857\n",
      "Recall: 0.8150406504065041\n",
      "Precision: 0.9925742574257427\n",
      "AUC: 0.979873973275639\n",
      "Checking class weight None and coefficient 0.016681005372000592\n",
      "Final Confusion Matrix\n",
      "[[2.8984e+03 6.0000e-01]\n",
      " [1.7400e+01 8.1000e+01]]\n",
      "F1 score: 0.9\n",
      "Recall: 0.823170731707317\n",
      "Precision: 0.9926470588235294\n",
      "AUC: 0.9802891535114469\n",
      "Checking class weight None and coefficient 0.027825594022071243\n",
      "Final Confusion Matrix\n",
      "[[2.8982e+03 8.0000e-01]\n",
      " [1.7200e+01 8.1200e+01]]\n",
      "F1 score: 0.9002217294900221\n",
      "Recall: 0.8252032520325203\n",
      "Precision: 0.9902439024390244\n",
      "AUC: 0.9806583408761501\n",
      "Checking class weight None and coefficient 0.046415888336127774\n",
      "Final Confusion Matrix\n",
      "[[2.8982e+03 8.0000e-01]\n",
      " [1.7200e+01 8.1200e+01]]\n",
      "F1 score: 0.9002217294900221\n",
      "Recall: 0.8252032520325203\n",
      "Precision: 0.9902439024390244\n",
      "AUC: 0.9809973783067955\n",
      "Checking class weight None and coefficient 0.0774263682681127\n",
      "Final Confusion Matrix\n",
      "[[2.898e+03 1.000e+00]\n",
      " [1.740e+01 8.100e+01]]\n",
      "F1 score: 0.8980044345898003\n",
      "Recall: 0.823170731707317\n",
      "Precision: 0.9878048780487805\n",
      "AUC: 0.9812745941509554\n",
      "Checking class weight None and coefficient 0.1291549665014884\n",
      "Final Confusion Matrix\n",
      "[[2.8976e+03 1.4000e+00]\n",
      " [1.6800e+01 8.1600e+01]]\n",
      "F1 score: 0.8996692392502755\n",
      "Recall: 0.8292682926829268\n",
      "Precision: 0.9831325301204819\n",
      "AUC: 0.9813873790811651\n",
      "Checking class weight None and coefficient 0.21544346900318834\n",
      "Final Confusion Matrix\n",
      "[[2.8974e+03 1.6000e+00]\n",
      " [1.7000e+01 8.1400e+01]]\n",
      "F1 score: 0.8974641675854466\n",
      "Recall: 0.8272357723577236\n",
      "Precision: 0.9807228915662651\n",
      "AUC: 0.9813952792171883\n",
      "Checking class weight None and coefficient 0.3593813663804626\n",
      "Final Confusion Matrix\n",
      "[[2.8972e+03 1.8000e+00]\n",
      " [1.6800e+01 8.1600e+01]]\n",
      "F1 score: 0.8976897689768977\n",
      "Recall: 0.8292682926829268\n",
      "Precision: 0.9784172661870504\n",
      "AUC: 0.9812648807523887\n",
      "Checking class weight None and coefficient 0.5994842503189409\n",
      "Final Confusion Matrix\n",
      "[[2.8972e+03 1.8000e+00]\n",
      " [1.6600e+01 8.1800e+01]]\n",
      "F1 score: 0.8989010989010988\n",
      "Recall: 0.83130081300813\n",
      "Precision: 0.9784688995215312\n",
      "AUC: 0.9810895809294271\n",
      "Checking class weight None and coefficient 1.0\n",
      "Final Confusion Matrix\n",
      "[[2.8972e+03 1.8000e+00]\n",
      " [1.6200e+01 8.2200e+01]]\n",
      "F1 score: 0.9013157894736842\n",
      "Recall: 0.8353658536585366\n",
      "Precision: 0.9785714285714286\n",
      "AUC: 0.9808823036881973\n",
      "Checking class weight balanced and coefficient 0.01\n",
      "Final Confusion Matrix\n",
      "[[2838.8   60.2]\n",
      " [   9.4   89. ]]\n",
      "F1 score: 0.7189014539579969\n",
      "Recall: 0.9044715447154471\n",
      "Precision: 0.5965147453083111\n",
      "AUC: 0.9800391223837732\n",
      "Checking class weight balanced and coefficient 0.016681005372000592\n",
      "Final Confusion Matrix\n",
      "[[2835.    64. ]\n",
      " [   9.2   89.2]]\n",
      "F1 score: 0.7090620031796503\n",
      "Recall: 0.9065040650406504\n",
      "Precision: 0.5822454308093995\n",
      "AUC: 0.9800228883508832\n",
      "Checking class weight balanced and coefficient 0.027825594022071243\n",
      "Final Confusion Matrix\n",
      "[[2833.8   65.2]\n",
      " [   9.    89.4]]\n",
      "F1 score: 0.7067193675889328\n",
      "Recall: 0.9085365853658537\n",
      "Precision: 0.5782664941785252\n",
      "AUC: 0.9799925464243209\n",
      "Checking class weight balanced and coefficient 0.046415888336127774\n",
      "Final Confusion Matrix\n",
      "[[2832.8   66.2]\n",
      " [   9.    89.4]]\n",
      "F1 score: 0.7039370078740158\n",
      "Recall: 0.9085365853658537\n",
      "Precision: 0.5745501285347043\n",
      "AUC: 0.9798996789387564\n",
      "Checking class weight balanced and coefficient 0.0774263682681127\n",
      "Final Confusion Matrix\n",
      "[[2830.6   68.4]\n",
      " [   9.    89.4]]\n",
      "F1 score: 0.6978922716627635\n",
      "Recall: 0.9085365853658537\n",
      "Precision: 0.5665399239543726\n",
      "AUC: 0.9798328904350685\n",
      "Checking class weight balanced and coefficient 0.1291549665014884\n",
      "Final Confusion Matrix\n",
      "[[2830.2   68.8]\n",
      " [   9.    89.4]]\n",
      "F1 score: 0.6968043647700702\n",
      "Recall: 0.9085365853658537\n",
      "Precision: 0.5651074589127687\n",
      "AUC: 0.9798044968768863\n",
      "Checking class weight balanced and coefficient 0.21544346900318834\n",
      "Final Confusion Matrix\n",
      "[[2830.    69. ]\n",
      " [   8.8   89.6]]\n",
      "F1 score: 0.6972762645914397\n",
      "Recall: 0.910569105691057\n",
      "Precision: 0.5649432534678436\n",
      "AUC: 0.9797790863133592\n",
      "Checking class weight balanced and coefficient 0.3593813663804626\n",
      "Final Confusion Matrix\n",
      "[[2829.2   69.8]\n",
      " [   8.8   89.6]]\n",
      "F1 score: 0.6951124903025602\n",
      "Recall: 0.910569105691057\n",
      "Precision: 0.5621079046424091\n",
      "AUC: 0.9797360977694097\n",
      "Checking class weight balanced and coefficient 0.5994842503189409\n",
      "Final Confusion Matrix\n",
      "[[2828.4   70.6]\n",
      " [   8.6   89.8]]\n",
      "F1 score: 0.6939721792890263\n",
      "Recall: 0.9126016260162602\n",
      "Precision: 0.559850374064838\n",
      "AUC: 0.9797412140141161\n",
      "Checking class weight balanced and coefficient 1.0\n",
      "Final Confusion Matrix\n",
      "[[2828.2   70.8]\n",
      " [   8.6   89.8]]\n",
      "F1 score: 0.6934362934362934\n",
      "Recall: 0.9126016260162602\n",
      "Precision: 0.5591531755915318\n",
      "AUC: 0.9797640006800801\n",
      "Best parameters found for Logistic Regression under AUC metric is None class weights and 0.21544346900318834 coefficient\n",
      "Best parameters found for Logistic Regression under F1 metric is None class weights and 0.01 coefficient\n",
      "Best parameters found for Logistic Regression under precision metric is None class weights and 0.016681005372000592 coefficient\n",
      "Best parameters found for Logistic Regression under recall metric is balanced class weights and 0.5994842503189409 coefficient\n"
     ]
    }
   ],
   "source": [
    "weights = [None,'balanced']\n",
    "coefficients = np.logspace(-2,0,10)\n",
    "fscore = []\n",
    "recall = []\n",
    "precision = []\n",
    "AU = []\n",
    "param_permuations = list(itertools.product(weights, coefficients))\n",
    "for cw,C in param_permuations:\n",
    "    print('Checking class weight {} and coefficient {}'.format(cw, C))\n",
    "    cf,au = train_logistic_regression(data_sc, folds, labels, cw, C)\n",
    "    TP = cf[1][1]\n",
    "    FP = cf[0][1]\n",
    "    FN = cf[1][0]\n",
    "    prec = TP/(TP+FP)\n",
    "    rec = TP/(TP+FN)\n",
    "    f1 = prec*rec*2/(prec+rec)\n",
    "    print('F1 score:', f1)\n",
    "    print('Recall:', rec)\n",
    "    print('Precision:', prec)\n",
    "    print('AUC:', au)\n",
    "    fscore.append(f1)\n",
    "    recall.append(rec)\n",
    "    precision.append(prec)\n",
    "    AU.append(au)\n",
    "\n",
    "idx = np.argmax(AU)\n",
    "best_params = param_permuations[idx]\n",
    "print('Best parameters found for Logistic Regression under AUC metric is {} class weights and {} coefficient'.format(best_params[0], best_params[1]))\n",
    "\n",
    "idx = np.argmax(f1)\n",
    "best_params = param_permuations[idx]\n",
    "print('Best parameters found for Logistic Regression under F1 metric is {} class weights and {} coefficient'.format(best_params[0], best_params[1]))\n",
    "\n",
    "idx = np.argmax(precision)\n",
    "best_params = param_permuations[idx]\n",
    "print('Best parameters found for Logistic Regression under precision metric is {} class weights and {} coefficient'.format(best_params[0], best_params[1]))\n",
    "\n",
    "idx = np.argmax(recall)\n",
    "best_params = param_permuations[idx]\n",
    "print('Best parameters found for Logistic Regression under recall metric is {} class weights and {} coefficient'.format(best_params[0], best_params[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking class weight None and coefficient 0.01\n",
      "Final Confusion Matrix\n",
      "[[102.6   0.4]\n",
      " [ 19.4  79. ]]\n",
      "F1 score: 0.8886389201349831\n",
      "Recall: 0.8028455284552845\n",
      "Precision: 0.9949622166246851\n",
      "AUC: 0.9794183163115202\n",
      "Checking class weight None and coefficient 0.016681005372000592\n",
      "Final Confusion Matrix\n",
      "[[102.6   0.4]\n",
      " [ 17.8  80.6]]\n",
      "F1 score: 0.8985507246376812\n",
      "Recall: 0.8191056910569106\n",
      "Precision: 0.9950617283950617\n",
      "AUC: 0.97957762687305\n",
      "Checking class weight None and coefficient 0.027825594022071243\n",
      "Final Confusion Matrix\n",
      "[[102.6   0.4]\n",
      " [ 15.4  83. ]]\n",
      "F1 score: 0.9130913091309131\n",
      "Recall: 0.8434959349593495\n",
      "Precision: 0.9952038369304556\n",
      "AUC: 0.9794967707589066\n",
      "Checking class weight None and coefficient 0.046415888336127774\n",
      "Final Confusion Matrix\n",
      "[[102.4   0.6]\n",
      " [ 13.8  84.6]]\n",
      "F1 score: 0.9215686274509804\n",
      "Recall: 0.8597560975609756\n",
      "Precision: 0.9929577464788734\n",
      "AUC: 0.9793186471411159\n",
      "Checking class weight None and coefficient 0.0774263682681127\n",
      "Final Confusion Matrix\n",
      "[[102.4   0.6]\n",
      " [ 12.6  85.8]]\n",
      "F1 score: 0.9285714285714286\n",
      "Recall: 0.8719512195121951\n",
      "Precision: 0.9930555555555556\n",
      "AUC: 0.9791000954662537\n",
      "Checking class weight None and coefficient 0.1291549665014884\n",
      "Final Confusion Matrix\n",
      "[[101.8   1.2]\n",
      " [ 11.6  86.8]]\n",
      "F1 score: 0.9313304721030042\n",
      "Recall: 0.8821138211382115\n",
      "Precision: 0.9863636363636363\n",
      "AUC: 0.9789816132395883\n",
      "Checking class weight None and coefficient 0.21544346900318834\n",
      "Final Confusion Matrix\n",
      "[[101.4   1.6]\n",
      " [ 10.8  87.6]]\n",
      "F1 score: 0.9339019189765458\n",
      "Recall: 0.8902439024390244\n",
      "Precision: 0.9820627802690584\n",
      "AUC: 0.9790208404632814\n",
      "Checking class weight None and coefficient 0.3593813663804626\n",
      "Final Confusion Matrix\n",
      "[[101.2   1.8]\n",
      " [ 10.2  88.2]]\n",
      "F1 score: 0.9363057324840763\n",
      "Recall: 0.8963414634146342\n",
      "Precision: 0.98\n",
      "AUC: 0.9789617994888452\n",
      "Checking class weight None and coefficient 0.5994842503189409\n",
      "Final Confusion Matrix\n",
      "[[100.8   2.2]\n",
      " [ 10.   88.4]]\n",
      "F1 score: 0.9354497354497354\n",
      "Recall: 0.8983739837398375\n",
      "Precision: 0.9757174392935982\n",
      "AUC: 0.9789225722651519\n",
      "Checking class weight None and coefficient 1.0\n",
      "Final Confusion Matrix\n",
      "[[100.2   2.8]\n",
      " [  9.8  88.6]]\n",
      "F1 score: 0.9336143308746048\n",
      "Recall: 0.9004065040650406\n",
      "Precision: 0.9693654266958425\n",
      "AUC: 0.978704220729186\n",
      "Checking class weight balanced and coefficient 0.01\n",
      "Final Confusion Matrix\n",
      "[[102.6   0.4]\n",
      " [ 19.   79.4]]\n",
      "F1 score: 0.8911335578002244\n",
      "Recall: 0.806910569105691\n",
      "Precision: 0.9949874686716791\n",
      "AUC: 0.9793394615863408\n",
      "Checking class weight balanced and coefficient 0.016681005372000592\n",
      "Final Confusion Matrix\n",
      "[[102.6   0.4]\n",
      " [ 17.4  81. ]]\n",
      "F1 score: 0.9010011123470523\n",
      "Recall: 0.823170731707317\n",
      "Precision: 0.995085995085995\n",
      "AUC: 0.9795183857597172\n",
      "Checking class weight balanced and coefficient 0.027825594022071243\n",
      "Final Confusion Matrix\n",
      "[[102.6   0.4]\n",
      " [ 15.4  83. ]]\n",
      "F1 score: 0.9130913091309131\n",
      "Recall: 0.8434959349593495\n",
      "Precision: 0.9952038369304556\n",
      "AUC: 0.979496970897803\n",
      "Checking class weight balanced and coefficient 0.046415888336127774\n",
      "Final Confusion Matrix\n",
      "[[102.4   0.6]\n",
      " [ 13.6  84.8]]\n",
      "F1 score: 0.9227421109902068\n",
      "Recall: 0.8617886178861789\n",
      "Precision: 0.9929742388758783\n",
      "AUC: 0.9792988333903729\n",
      "Checking class weight balanced and coefficient 0.0774263682681127\n",
      "Final Confusion Matrix\n",
      "[[102.    1. ]\n",
      " [ 12.4  86. ]]\n",
      "F1 score: 0.9277238403451996\n",
      "Recall: 0.8739837398373983\n",
      "Precision: 0.9885057471264368\n",
      "AUC: 0.9790802817155105\n",
      "Checking class weight balanced and coefficient 0.1291549665014884\n",
      "Final Confusion Matrix\n",
      "[[101.8   1.2]\n",
      " [ 11.2  87.2]]\n",
      "F1 score: 0.9336188436830835\n",
      "Recall: 0.8861788617886178\n",
      "Precision: 0.9864253393665158\n",
      "AUC: 0.9788831449025623\n",
      "Checking class weight balanced and coefficient 0.21544346900318834\n",
      "Final Confusion Matrix\n",
      "[[101.2   1.8]\n",
      " [ 10.8  87.6]]\n",
      "F1 score: 0.9329073482428115\n",
      "Recall: 0.8902439024390244\n",
      "Precision: 0.9798657718120806\n",
      "AUC: 0.9790208404632814\n",
      "Checking class weight balanced and coefficient 0.3593813663804626\n",
      "Final Confusion Matrix\n",
      "[[101.2   1.8]\n",
      " [ 10.   88.4]]\n",
      "F1 score: 0.9374337221633087\n",
      "Recall: 0.8983739837398375\n",
      "Precision: 0.9800443458980045\n",
      "AUC: 0.9789619996277417\n",
      "Checking class weight balanced and coefficient 0.5994842503189409\n",
      "Final Confusion Matrix\n",
      "[[100.6   2.4]\n",
      " [ 10.   88.4]]\n",
      "F1 score: 0.9344608879492601\n",
      "Recall: 0.8983739837398375\n",
      "Precision: 0.9735682819383259\n",
      "AUC: 0.9788237036503332\n",
      "Checking class weight balanced and coefficient 1.0\n",
      "Final Confusion Matrix\n",
      "[[100.2   2.8]\n",
      " [  9.6  88.8]]\n",
      "F1 score: 0.9347368421052631\n",
      "Recall: 0.9024390243902439\n",
      "Precision: 0.9694323144104804\n",
      "AUC: 0.9787238343410326\n",
      "Best parameters found for Logistic Regression under AUC metric is None class weights and 0.016681005372000592 coefficient\n",
      "Best parameters found for Logistic Regression under F1 metric is None class weights and 0.01 coefficient\n",
      "Best parameters found for Logistic Regression under precision metric is None class weights and 0.027825594022071243 coefficient\n",
      "Best parameters found for Logistic Regression under recall metric is balanced class weights and 1.0 coefficient\n"
     ]
    }
   ],
   "source": [
    "#Notice that if we fully downsample the dataset so that valid and fraud are equally represented, we have much more consistent metrics.\n",
    "#Then we see that the recommended parameters begin to converge.\n",
    "\n",
    "csv_data2 = [row for row in csv_data if int(row[-1]) == 1 or random.random() < 0.00164/.05]\n",
    "\n",
    "# Format: Time, V1, ..., V28, Amount, Class\n",
    "data2 = np.array([[float(x) for x in row[1:-1]] for row in csv_data2])\n",
    "times2 = np.array([float(row[0]) for row in csv_data2])\n",
    "labels2 = np.array([int(row[-1]) for row in csv_data2])\n",
    "\n",
    "data_sc2 = scaler.fit_transform(data2)\n",
    "\n",
    "fraud_indices2 = np.where(labels2 == 1)[0]\n",
    "np.random.shuffle(fraud_indices2)\n",
    "fraud_splits2 = np.array_split(fraud_indices2, 5)\n",
    "\n",
    "valid_indices2 = np.where(labels2 == 0)[0]\n",
    "np.random.shuffle(valid_indices2)\n",
    "valid_splits2 = np.array_split(valid_indices2, 5)\n",
    "\n",
    "folds2 = [np.concatenate((fraud_sp, valid_sp)) for fraud_sp, valid_sp in zip(fraud_splits2, valid_splits2)]\n",
    "\n",
    "weights = [None,'balanced']\n",
    "coefficients = np.logspace(-2,0,10)\n",
    "fscore = []\n",
    "recall = []\n",
    "precision = []\n",
    "AU = []\n",
    "param_permuations = list(itertools.product(weights, coefficients))\n",
    "for cw,C in param_permuations:\n",
    "    print('Checking class weight {} and coefficient {}'.format(cw, C))\n",
    "    cf,au = train_logistic_regression(data_sc2, folds2, labels2, cw, C)\n",
    "    TP = cf[1][1]\n",
    "    FP = cf[0][1]\n",
    "    FN = cf[1][0]\n",
    "    prec = TP/(TP+FP)\n",
    "    rec = TP/(TP+FN)\n",
    "    f1 = prec*rec*2/(prec+rec)\n",
    "    print('F1 score:', f1)\n",
    "    print('Recall:', rec)\n",
    "    print('Precision:', prec)\n",
    "    print('AUC:', au)\n",
    "    fscore.append(f1)\n",
    "    recall.append(rec)\n",
    "    precision.append(prec)\n",
    "    AU.append(au)\n",
    "\n",
    "idx = np.argmax(AU)\n",
    "best_params = param_permuations[idx]\n",
    "print('Best parameters found for Logistic Regression under AUC metric is {} class weights and {} coefficient'.format(best_params[0], best_params[1]))\n",
    "\n",
    "idx = np.argmax(f1)\n",
    "best_params = param_permuations[idx]\n",
    "print('Best parameters found for Logistic Regression under F1 metric is {} class weights and {} coefficient'.format(best_params[0], best_params[1]))\n",
    "\n",
    "idx = np.argmax(precision)\n",
    "best_params = param_permuations[idx]\n",
    "print('Best parameters found for Logistic Regression under precision metric is {} class weights and {} coefficient'.format(best_params[0], best_params[1]))\n",
    "\n",
    "idx = np.argmax(recall)\n",
    "best_params = param_permuations[idx]\n",
    "print('Best parameters found for Logistic Regression under recall metric is {} class weights and {} coefficient'.format(best_params[0], best_params[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# kNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_KNN(data, k, scale=False):\n",
    "    confusion_mat = np.zeros((2, 2))\n",
    "    AU = 0\n",
    "    errors = 0\n",
    "    for i in range(5):\n",
    "        knn = KNeighborsClassifier(n_neighbors=k)\n",
    "        \n",
    "        train_data = np.delete(data, folds[i], axis=0)\n",
    "        test_data = data[folds[i]]\n",
    "        if scale:\n",
    "            scaler = StandardScaler()\n",
    "            train_data = scaler.fit_transform(train_data)\n",
    "            test_data = scaler.transform(test_data)\n",
    "\n",
    "        knn.fit(train_data, np.delete(labels, folds[i]))\n",
    "        pred = knn.predict(test_data)\n",
    "\n",
    "        conf_mat = confusion_matrix(labels[folds[i]], pred)\n",
    "#         print('Fold', i)\n",
    "#         print(conf_mat)\n",
    "        fpr, tpr, thresholds = roc_curve(labels[folds[i]], pred)\n",
    "        area_under_curve = auc(fpr, tpr)\n",
    "        AU += area_under_curve\n",
    "#         print('Fold', i)\n",
    "#         print(conf_mat)\n",
    "#         print('Area under curve:', area_under_curve)\n",
    "        err = sum([1 if a != b else 0 for a, b in zip(labels[folds[i]], pred)]) / len(pred)\n",
    "        errors += err\n",
    "\n",
    "        confusion_mat += conf_mat\n",
    "\n",
    "    confusion_mat /= 5\n",
    "    AU /= 5\n",
    "    errors /= 5\n",
    "#     print('Final Confusion Matrix')\n",
    "#     print(confusion_mat)\n",
    "    \n",
    "#     fnr = confusion_mat[0, 1] / sum(confusion_mat[0, :])\n",
    "#     fpr = confusion_mat[1, 0] / sum(confusion_mat[1,:])\n",
    "#     print('False negatives (valid):', fnr)\n",
    "#     print('False positives (fraud):', fpr)\n",
    "    \n",
    "#     metric = (fnr * (len(labels) - sum(labels)) + fpr * sum(labels)) / len(labels)\n",
    "    return confusion_mat, AU, errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "K = 1, errors = 0.009875096354971826\n",
      "Confusion Matrix:\n",
      "[[2889.4    9.6]\n",
      " [  20.    78.4]]\n",
      "False Negative Rate: 0.003311\n",
      "False Positive Rate: 0.203252\n",
      "fscore: 0.8412017167381974\n",
      "\n",
      "K = 3, errors = 0.00874080662828717\n",
      "Confusion Matrix:\n",
      "[[2.8972e+03 1.8000e+00]\n",
      " [2.4400e+01 7.4000e+01]]\n",
      "False Negative Rate: 0.000621\n",
      "False Positive Rate: 0.247967\n",
      "fscore: 0.8495981630309988\n",
      "\n",
      "K = 5, errors = 0.009875118614278052\n",
      "Confusion Matrix:\n",
      "[[2.8976e+03 1.4000e+00]\n",
      " [2.8200e+01 7.0200e+01]]\n",
      "False Negative Rate: 0.000483\n",
      "False Positive Rate: 0.286585\n",
      "fscore: 0.8258823529411765\n",
      "\n",
      "K = 7, errors = 0.01074251925930823\n",
      "Confusion Matrix:\n",
      "[[2.8978e+03 1.2000e+00]\n",
      " [3.1000e+01 6.7400e+01]]\n",
      "False Negative Rate: 0.000414\n",
      "False Positive Rate: 0.315041\n",
      "fscore: 0.8071856287425151\n",
      "\n",
      "K = 9, errors = 0.011142986437627308\n",
      "Confusion Matrix:\n",
      "[[2.8986e+03 4.0000e-01]\n",
      " [3.3000e+01 6.5400e+01]]\n",
      "False Negative Rate: 0.000138\n",
      "False Positive Rate: 0.335366\n",
      "fscore: 0.7965895249695493\n",
      "\n",
      "K = 11, errors = 0.011676786860242497\n",
      "Confusion Matrix:\n",
      "[[2.8986e+03 4.0000e-01]\n",
      " [3.4600e+01 6.3800e+01]]\n",
      "False Negative Rate: 0.000138\n",
      "False Positive Rate: 0.351626\n",
      "fscore: 0.7847478474784748\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for k in [1, 3, 5, 7, 9, 11]:\n",
    "    conf, au, errors = train_KNN(data, k)\n",
    "    print('K = {}, errors = {}'.format(k, errors))\n",
    "    print('Confusion Matrix:')\n",
    "    print(conf)\n",
    "#     print('AUC: {}'.format(au))\n",
    "#     print('recall: {}'.format(conf[1, 0] / sum(conf[:, 0])))\n",
    "    fnr = conf[0, 1] / sum(conf[0, :])\n",
    "    fpr = conf[1, 0] / sum(conf[1,:])\n",
    "    print('False Negative Rate: {:3f}\\nFalse Positive Rate: {:3f}'.format(fnr, fpr))\n",
    "#     metric = (fnr * (len(labels) - sum(labels)) + fpr * sum(labels)) / len(labels)\n",
    "#     print('metric: {}'.format(metric))\n",
    "    fscore = 2*conf[1,1] / (2*conf[1,1] + conf[0,1] + conf[1,0])\n",
    "    print('fscore: {}'.format(fscore))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
